{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced Evolutionary Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We have already considered the many qualitative and quantitative parameters and design choices when implementing evolutionary algorithms. In this chapter, we will consider some more fundamental changes to the algorithm, such as hybridisation of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "# For presenting as slides\n",
    "#plt.rcParams['figure.figsize'] = [12, 8]\n",
    "#plt.rcParams.update({'font.size': 22})\n",
    "#plt.rcParams['lines.linewidth'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since an improvement to a solution does not give us any guarantees that we are in fact moving towards the optimum, evolutionary algorithms allow non-local moves. However, evolution may take longer to fine-tune solutions, while local search does exactly this very efficiently. The term _Memetic Algorithms_ is commonly used to denote the combination of global and local search. The term sometimes also refers to the use of instance-specific knowledge in search operators, but we will focus on the combination of global and local search in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As an example problem we consider a variation of the one-max problem, such that the search operators are simplistic, and we understand the search landscape well. We are still trying to optimise the bitstring to contain all `1`s, but assume we have a somewhat more complex fitness landscape. Given a bitstring encoding, let the fitness function be the following _hurdle_ function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$f(x) = - \\Big\\lceil \\frac{z(x)}{w} \\Big\\rceil - \\frac{rem(z(x), w)}{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here $z(x)$ is the number of zeros in the bitstring $x$; $w \\in \\{ 2, 3, \\ldots, n\\}$ is the hurdle width; $rem(z(x), w)$ is the remainder of $z(x)$ divided by $w$, and $\\lceil \\cdot \\rceil$ is the ceiling function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It will be easiest to understand what this does by looking at the resulting fitness landscape over a limited input range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "values = []\n",
    "for z in range(0, 100):\n",
    "    values.append(-math.ceil(z/w) - (z % w)/w)\n",
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The optimal value has the fitness value $0$ (i.e. $z(0) = 0$ implies all bits are $1$). The representation is our usual bitstring, and we will use a moderate length for our bitstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use our usual helper function to wrap lists in order to enable us to cache fitness values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class L(list):\n",
    "    \"\"\"\n",
    "    A subclass of list that can accept additional attributes.\n",
    "    Should be able to be used just like a regular list.\n",
    "    \"\"\"\n",
    "    def __new__(self, *args, **kwargs):\n",
    "        return super(L, self).__new__(self, args, kwargs)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if len(args) == 1 and hasattr(args[0], '__iter__'):\n",
    "            list.__init__(self, args[0])\n",
    "        else:\n",
    "            list.__init__(self, args)\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As usual, an individual is a random sequence of bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_solution():\n",
    "    domain = [0,1]\n",
    "    return L([random.choice(domain) for _ in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The fitness function calculates the hurdle function. In order to avoid redundantly calculating fitness values, we first check if the individual already has a cached fitness value, and if so, we simply return that. If we do have to calculate a new fitness value then we append an item to our `fitness_values` list; we will use this list to store the best fitness value seen to date, so that we can plot the result afterwards. We will also use it as a stopping condition, since it reliably keeps track of how many times the fitness was actually _calculated_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fitness_values = []\n",
    "\n",
    "def get_fitness(solution):\n",
    "    if hasattr(solution, \"fitness\"):\n",
    "        return solution.fitness\n",
    "    \n",
    "    z = len(solution) - sum(solution)\n",
    "    fitness = -math.ceil(z/w) - (z % w)/w\n",
    "    solution.fitness = fitness\n",
    "    \n",
    "    if not fitness_values:\n",
    "        fitness_values.append(fitness)\n",
    "    else: \n",
    "        fitness_values.append(max(fitness, fitness_values[-1]))\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_fitness(get_random_solution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's first consider how local search fares with our hurdle problem. We define a neighbourhood and recall our basic hillclimber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_neighbours(candidate):\n",
    "    neighbours = []\n",
    "    for pos in range(len(candidate)):\n",
    "        copy = L(candidate[:])\n",
    "        copy[pos] = 1 - copy[pos]\n",
    "        neighbours.append(copy)\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As a small change to prior implementations, we will use the `fitness_values` list as a stopping condition, and make sure that we don't exceed `max_steps` fitness evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "max_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def hillclimbing_restart():\n",
    "    fitness_values.clear()\n",
    "    current = get_random_solution()\n",
    "    best = L(current[:])\n",
    "    best_fitness = get_fitness(current)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "\n",
    "        best_neighbour = None\n",
    "        neighbour_fitness = -sys.maxsize\n",
    "        for neighbour in get_neighbours(current):\n",
    "            fitness = get_fitness(neighbour)\n",
    "\n",
    "            if fitness > neighbour_fitness:\n",
    "                best_neighbour = neighbour\n",
    "                neighbour_fitness = fitness\n",
    "\n",
    "        # Random restart if no neighbour is better\n",
    "        if neighbour_fitness <= get_fitness(current):\n",
    "            current = get_random_solution()\n",
    "            neighbour_fitness = get_fitness(current)\n",
    "        else:\n",
    "            current = best_neighbour                        \n",
    "\n",
    "        if neighbour_fitness > best_fitness:\n",
    "            best = L(current[:])\n",
    "            best_fitness = neighbour_fitness\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hillclimbing_restart()\n",
    "plt.plot(fitness_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The hillclimber is quite unlikely to reach the optimal solution: The fitness landscape contains many local optima, and a random (re-)start needs to luckily jump beyond the last hurdle for the gradient to point to the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's compare this to a basic evolutionary search algorithm; to keep things simple initially we will just use a (1+1)EA, so we just need to add a mutation function that implements the usual probabilistic bitflips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mutate(solution):\n",
    "    P_mutate = 1/len(solution)\n",
    "    mutated = L(solution[:])\n",
    "    for position in range(len(solution)):\n",
    "        if random.random() < P_mutate:\n",
    "            mutated[position] = 1 - mutated[position]\n",
    "    return mutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def oneplusoneea():\n",
    "    fitness_values.clear()\n",
    "    current = get_random_solution()\n",
    "    fitness = get_fitness(current)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "        candidate = mutate(current)\n",
    "        candidate_fitness = get_fitness(candidate)\n",
    "        if candidate_fitness >= fitness:\n",
    "            fitness = candidate_fitness\n",
    "            current = candidate\n",
    "\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "oneplusoneea()\n",
    "plt.plot(fitness_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The hurdles are problematic also for this global search algorithm: The search not only needs to jump over all hurdles, but it also needs to jump higher than the preceding hurdles. Again the result tends to be that the algorithm does not find an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A memetic algorithm combines these two algorithms, and can overcome these problems: The global search can effectively jump over hurdles, and the local search can climb the next hurdle. As an initial memetic algorithm we create a simple (1+1)MA. For this we will adapt the hillclimber so that it takes a starting point of the search as a parameter. To avoid spending too much time exploring the neighbourhood, we will also make this a first-ascent hillclimber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hillclimbing(starting_point):\n",
    "\n",
    "    current =  starting_point\n",
    "    fitness = get_fitness(current)\n",
    "\n",
    "    improved = True\n",
    "    while improved and len(fitness_values) < max_steps:\n",
    "        improved = False\n",
    "        \n",
    "        for neighbour in get_neighbours(current):\n",
    "            neighbour_fitness = get_fitness(neighbour)\n",
    "\n",
    "            if neighbour_fitness > fitness:\n",
    "                current = neighbour\n",
    "                fitness = neighbour_fitness\n",
    "                improved = True\n",
    "                break\n",
    "        \n",
    "    return current, fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The (1+1)MA now applies local search after each mutation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def oneplusonema():\n",
    "    fitness_values.clear()\n",
    "    current = get_random_solution()\n",
    "    fitness = get_fitness(current)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "        candidate = mutate(current)\n",
    "        candidate, candidate_fitness = hillclimbing(candidate)\n",
    "        if candidate_fitness >= fitness:\n",
    "            fitness = candidate_fitness\n",
    "            current = candidate\n",
    "\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now let's see how the three algorithms compare on our hurdle problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hillclimbing_restart()\n",
    "hc_values = fitness_values[:]\n",
    "\n",
    "oneplusoneea()\n",
    "opo_values = fitness_values[:]\n",
    "\n",
    "oneplusonema()\n",
    "opoma_values = fitness_values[:]\n",
    "\n",
    "plt.ylabel('Fitness', fontsize=15)\n",
    "plt.plot(hc_values, label = \"Hillclimbing\")\n",
    "plt.plot(opo_values, label = \"(1+1)EA\")\n",
    "plt.plot(opoma_values, label = \"(1+1)MA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The local improvement actually reflects Lamarckian evolution theory, as individuals can change their own genotype. An alternative way to implement a memetic algorithm would be to exploit the _Baldwin effect_: The genotype is not improved by the local search, but the fitness is evaluated on an improved genotype. The Baldwin effect describes how the preference of the locally improvable individuals leads to this establishing in the genotype eventually through evolution. However, when implementing algorithms we are not bound by biological realisties, so we can just implement Lamarckism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When generalising our (1+1)MA to population-based memetic algorithms we are faced with a new parameter: Applying local improvement on _all_ offspring is intuitively very computationally expensive, as we need to explore the neighbourhoods of (potentially) many individuals. The local improvement is therefor usually only done probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We first need all the components of a regular genetic algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"P_xover\": 0.7,\n",
    "    \"population_size\": 20,\n",
    "    \"tournament_size\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tournament_selection(population):\n",
    "    # Make sure the sample isn't larger than the population\n",
    "    candidates = random.sample(population, min(len(population), configuration[\"tournament_size\"]))\n",
    "    winner = max(candidates, key=lambda x: get_fitness(x))    \n",
    "                \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crossover(parent1, parent2):\n",
    "    pos = random.randint(0, len(parent1))\n",
    "    offspring1 = L(parent1[:pos] + parent2[pos:])\n",
    "    offspring2 = L(parent2[:pos] + parent1[pos:])\n",
    "    return offspring1, offspring2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ga():\n",
    "    fitness_values.clear()\n",
    "    population = [get_random_solution() for _ in range(configuration[\"population_size\"])]\n",
    "    best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "        new_population = []\n",
    "\n",
    "        while len(new_population) < len(population):\n",
    "            parent1 = tournament_selection(population)\n",
    "            parent2 = tournament_selection(population)\n",
    "\n",
    "            if random.random() < configuration[\"P_xover\"]:\n",
    "                offspring1, offspring2 = crossover(parent1, parent2)\n",
    "            else:\n",
    "                offspring1, offspring2 = parent1, parent2\n",
    "\n",
    "            offspring1 = mutate(offspring1)\n",
    "            offspring2 = mutate(offspring2)\n",
    "\n",
    "            new_population.append(offspring1)\n",
    "            new_population.append(offspring2)\n",
    "\n",
    "        population.clear()\n",
    "        population.extend(new_population)\n",
    "\n",
    "        best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "        best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    return best_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ga()\n",
    "ga_values = fitness_values[:]\n",
    "\n",
    "plt.ylabel('Fitness', fontsize=15)\n",
    "plt.plot(hc_values, label = \"Hillclimbing\")\n",
    "plt.plot(opo_values, label = \"(1+1)EA\")\n",
    "plt.plot(opoma_values, label = \"(1+1)MA\")\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The memetic algorithm differs only by one aspect -- the probabilistic choice of applying local search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "configuration[\"P_localsearch\"] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ma():\n",
    "    fitness_values.clear()\n",
    "    population = [get_random_solution() for _ in range(configuration[\"population_size\"])]\n",
    "    best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "        new_population = []\n",
    "\n",
    "        while len(new_population) < len(population):\n",
    "            parent1 = tournament_selection(population)\n",
    "            parent2 = tournament_selection(population)\n",
    "\n",
    "            if random.random() < configuration[\"P_xover\"]:\n",
    "                offspring1, offspring2 = crossover(parent1, parent2)\n",
    "            else:\n",
    "                offspring1, offspring2 = parent1, parent2\n",
    "\n",
    "            offspring1 = mutate(offspring1)\n",
    "            offspring2 = mutate(offspring2)\n",
    "\n",
    "            if random.random() < configuration[\"P_localsearch\"]:\n",
    "                offspring1, offspring1_fitness = hillclimbing(offspring1)\n",
    "                offspring2, offspring2_fitness = hillclimbing(offspring2)\n",
    "\n",
    "            new_population.append(offspring1)\n",
    "            new_population.append(offspring2)\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "        best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "        best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    return best_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As a baseline for our experiments, we will also use a random search as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def randomsearch():\n",
    "    fitness_values.clear()\n",
    "    best = get_random_solution()\n",
    "    best_fitness = get_fitness(best)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "        candidate = get_random_solution()\n",
    "        fitness = get_fitness(candidate)\n",
    "        if fitness > best_fitness:\n",
    "            best = candidate\n",
    "            best_fitness = fitness\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ga()\n",
    "ga_values = fitness_values[:]\n",
    "\n",
    "ma()\n",
    "ma_values = fitness_values[:]\n",
    "\n",
    "randomsearch()\n",
    "random_values = fitness_values[:]\n",
    "\n",
    "plt.ylabel('Fitness', fontsize=15)\n",
    "plt.plot(hc_values, label = \"Hillclimbing\")\n",
    "plt.plot(opo_values, label = \"(1+1)EA\")\n",
    "plt.plot(opoma_values, label = \"(1+1)MA\")\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.plot(ma_values, label = \"MA\")\n",
    "plt.plot(random_values, label = \"Random\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Individual runs may vary (and we will do a more systematic comparison later on), but generally the MA-variants tend to find optimal solutions, while the others do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While the memetic versions seem to have an edge over the non-memetic versions, the performance is dependent on the size of the neighbourhood: For example, if we use a large `n` in a bitstring representation, then the neighbourhood may become very large, and the exploration used in the local search may be very expensive. An upper bound on the number of fitness evaluations could prevent the local search from wasting the search budget in those cases where the local search does not actually contribute to improving the solution. We can investigate the effect of the neighbourhood size by re-running the search for different values of `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "\n",
    "def run_times(algorithm, repetitions):\n",
    "    global fitness_values\n",
    "    result = []\n",
    "    for i in range(repetitions):\n",
    "        with io.capture_output() as captured: \n",
    "            algorithm()\n",
    "        result.append(fitness_values[-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(18, 4))\n",
    "\n",
    "num_plot = 0\n",
    "for n in [100, 200, 300, 400, 500]:\n",
    "    results = {\n",
    "        \"Standard GA\"  : run_times(ga, 10),\n",
    "        \"MA\"           : run_times(ma, 10)\n",
    "    }\n",
    "    axes[num_plot].boxplot(results.values())\n",
    "    axes[num_plot].set_title(f\"n = {n}\")\n",
    "    axes[num_plot].set_xticklabels(results.keys())\n",
    "    num_plot += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As a point for comparison, let's also consider the standard one-max problem for the same sizes, where we should see a bit more spread in terms of fitness values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hurdle_fitness = get_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_fitness(solution):\n",
    "    if hasattr(solution, \"fitness\"):\n",
    "        return solution.fitness\n",
    "    \n",
    "    fitness = sum(solution)\n",
    "    solution.fitness = fitness\n",
    "    \n",
    "    if not fitness_values:\n",
    "        fitness_values.append(fitness)\n",
    "    else:\n",
    "        fitness_values.append(max(fitness, fitness_values[-1]))\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "onemax_fitness = get_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(18, 4))\n",
    "\n",
    "num_plot = 0\n",
    "for n in [100, 200, 300, 400, 500]:\n",
    "    results = {\n",
    "        \"Standard GA\"  : run_times(ga, 10),\n",
    "        \"MA\"           : run_times(ma, 10)\n",
    "    }\n",
    "    axes[num_plot].boxplot(results.values())\n",
    "    axes[num_plot].set_title(f\"n = {n}\")\n",
    "    axes[num_plot].set_xticklabels(results.keys())\n",
    "    num_plot += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Restore \n",
    "get_fitness = hurdle_fitness\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Evolutionary Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The easiest way to parallelise evolutionary algorithms is to conduct _independent runs_. This requires no adaptation at all, and the probability of finding an optimal solution increases with the number of parallel runs. _Master-slave_ models represent a slight modification of evolutionary search, which exploits the fact that reproduction and fitness evaluation of a population is an embarrassingly parallel problem: A master distributes the work of producing and evaluating offspring for the next population to a number of slaves. The master then waits until all slaves have completed, and continues with the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The _island model_ extends the notion of independent runs: There are independent sub-populations (islands); these islands evolve independently, but periodically exchange individuals (migration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "An island model implies a number of parameters: \n",
    "- An emigration policy which decides whether migrants are removed from the source population, or whether only copies migrate (pollination).\n",
    "- A selection policy which decides which individuals emigrate (e.g. best, worst, or random)\n",
    "- An immigration policy which decides how immigrants are integrated (e.g. replace worst individual, random individuals, or use standard selection mechanism for replacement)\n",
    "- A migration interval or frequency which decides how often and when migration takes place\n",
    "- The migration size decides how many individuals can migrate\n",
    "- A migration topology decides which islands may exchange individuals. This is typically handled using directed graphs with islands as nodes and directed edges connecting islands; iondividuals can migrate only along edges of the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As an example, we will let the best individuals emigrate, but use pollination, such that only copies migrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "configuration[\"emigration_size\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def emigrate(population):\n",
    "    sorted_population = sorted(population, key=lambda k: get_fitness(k), reverse=True)\n",
    "    return sorted_population[:configuration[\"emigration_size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given $n$ immigrants we replace the worst $n$ individuals of the population with the immigrants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def immigrate(population, immigrants):\n",
    "    population.sort(key=lambda k: get_fitness(k))\n",
    "    for pos in range(len(immigrants)):\n",
    "        population[pos] = immigrants[pos]\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use a simple ring topology where each island can only send immigrants to the next island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "configuration[\"num_islands\"] = 5\n",
    "configuration[\"population_size\"] = 50\n",
    "configuration[\"island_size\"] = configuration[\"population_size\"] // configuration[\"num_islands\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Each subpopulation evolves independently using the standard evolution mechanisms, so we can extract the usual evolution step function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evolution_step(population):\n",
    "    new_population = []\n",
    "    while len(new_population) < len(population):\n",
    "        parent1 = tournament_selection(population)\n",
    "        parent2 = tournament_selection(population)\n",
    "\n",
    "        if random.random() < configuration[\"P_xover\"]:\n",
    "            offspring1, offspring2 = crossover(parent1, parent2)\n",
    "        else:\n",
    "            offspring1, offspring2 = parent1, parent2\n",
    "\n",
    "        offspring1 = mutate(offspring1)\n",
    "        offspring2 = mutate(offspring2)\n",
    "\n",
    "        new_population.append(offspring1)\n",
    "        new_population.append(offspring2)\n",
    "\n",
    "    population.clear()\n",
    "    population.extend(new_population)\n",
    "\n",
    "    best_fitness = max([get_fitness(k) for k in population])\n",
    "    return best_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The GA itself is now responsible for creating the islands, and for controlling the migration between the islands according to the migration parameters. There is one major limitation to our implementation: We will not use multi-threading to actually let the islands evolve independently. While this would be easy in principle, the comparison with other algorithm versions would become moretricky. Thus, our implementation will just evaluate the effects of subpopulations and migration, and if you want to speed things up, you can extend the implementation to let the islands evolve in their own threads or processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_ga():\n",
    "    fitness_values.clear()\n",
    "    islands = []\n",
    "    for i in range(configuration[\"num_islands\"]):\n",
    "        islands.append([get_random_solution() for _ in range(configuration[\"island_size\"])])\n",
    "\n",
    "    best_solution = max([y for x in islands for y in x], key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "\n",
    "        for i in range(configuration[\"num_islands\"]):\n",
    "            migrants = emigrate(islands[i])\n",
    "            target = i + 1 if i < configuration[\"num_islands\"] - 1 else 0\n",
    "            immigrate(islands[target], migrants)\n",
    "\n",
    "        # This could be actually parallelised:\n",
    "        for i in range(configuration[\"num_islands\"]):\n",
    "            island_fitness = evolution_step(islands[i])\n",
    "            if island_fitness > best_fitness:\n",
    "                best_fitness = island_fitness\n",
    "\n",
    "    best = max([y for x in islands for y in x], key=lambda k: get_fitness(k))\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To see how the migration affects the search, let's briefly move away from our complicated hurdle fitness landscape to the easier one-max problem with its nice gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "get_fitness = onemax_fitness\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ga()\n",
    "values_ga = fitness_values[:]\n",
    "\n",
    "parallel_ga()\n",
    "values_pga = fitness_values[:]\n",
    "\n",
    "plt.plot(values_ga, label=\"GA\")\n",
    "plt.plot(values_pga, label=\"Parallel GA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There usually are two things noticable in the plot: First, evolution may initially be a bit slower since we set the island population size the same as the population size, so there's more to evaluate initially. However, over time the effects should show how the migration leads to an overall higher fitness value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Cellular evolutionary algorithms, which we considered in a previous chapter, are a special case of an island model with more fine-grained form of parallelisation: \n",
    "Each island (cell) contains only a single individual. During reproduction, an individual is only allowed to mate with its neighbours, as defined by the topology (commonly rings, grids, or two-dimensional torus graphs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation of Distribution Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In a genetic algorithm we select (stochastically) from the population, which, technically, is akin to sampling from an unknown distribution. The idea of Estimation of Distribution Algorithms (EDA) is to replace the population with a _model_ of that distribution, and the stochastic operators are then replaced with repeated estimation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "An EDA consists of:\n",
    "- A selection operator for selecting promising solutions (same as in GA)\n",
    "- An assumed class of probabilistic models to use for modelling and sampling\n",
    "- A procedure for learning a probabilistic model for the selected solutions\n",
    "- A procedure for sampling the built probabilistic model\n",
    "- A replacement operator for combining the populations of old and new candidate solutions (same as in GA)\n",
    "\n",
    "Different instances of EDAs differ mainly in the class of probabilistic models and the procedures used for evaluating candidates models and searching for a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are two main classes of EDAs: Those based on an explicit population, and algorithms that are purely incremental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compact Genetic Agorithm (CGA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "A compact genetic algorithm (CGA) is an incremental EDA. We start with an initial probability distribution (e.g., randomly initialised, or uniform probabilities). We then sample solutions from that distribution, and select the best and the worst of the sample set. These two are then used to update the probability distribution to make it the better individual more likely in that distribution, and the worse individual less likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As a first step we need to decide on a probabilistic model to represent our solution space (bitstrings). A very simple model would be to assign each bit in our bitstring of length `n` a probability of this bit being 1. Thus, if we want to draw a sample from the distribution we would iterate over the `n` probabilities, and append a `1` at position $i$ with the probability $p_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    solution = []\n",
    "    for p in distribution:\n",
    "        if random.random() < p:\n",
    "            solution.append(1)\n",
    "        else:\n",
    "            solution.append(0)\n",
    "\n",
    "    return L(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will start with all probabilities $p_i$ set to 0.5, such that sampled solutions are completely random at first. At each step, given the better sampled solution $x$ and the worse sampled solution $y$, we update the probability for each bit as follows for each bit $i$:\n",
    "- If $x_i = y_i$ then the probability of bit $i$ is unchanged\n",
    "- If $x_i - y_i = 1$ then the probability of bit $i$ being 1 is increased, since $x_i$ was 1 and $y_i$ was 0.\n",
    "- If $x_i - y_i = -1$ then the probability of bit $i$ being 1 is decreased, since $x_i$ was 0 and $y_i$ was 1.\n",
    "\n",
    "The rate at which the probability is updated depends on a parameter $\\lambda$, which is essentially $1/N$ for a hypothetical population size $N$. Thus, the probability update function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p_{t+1}(i) = p_t(i) + \\lambda \\cdot (x_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We thus need to set our parameters. Besides $\\lambda$, we also need to decide how many samples to draw. The number of samples essentially represents the selective pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "# -> Selection pressure\n",
    "configuration[\"num_samples\"] = 2\n",
    "\n",
    "# Learning rate ( 1 / N -- Hypothetical population size)\n",
    "configuration[\"lambda\"] = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cga():\n",
    "    fitness_values.clear()\n",
    "    distribution = [0.5 for _ in range(n)]\n",
    "    best_fitness = -sys.maxsize\n",
    "    best_individual = None\n",
    "    while len(fitness_values) < max_steps:\n",
    "        candidates = [sample(distribution) for _ in range(configuration[\"num_samples\"])]\n",
    "        best  = max(candidates, key=lambda k: get_fitness(k))\n",
    "        worst = min(candidates, key=lambda k: get_fitness(k))\n",
    "\n",
    "        for i in range(n):\n",
    "            distribution[i] = distribution[i] + configuration[\"lambda\"] * (best[i] - worst[i])\n",
    "\n",
    "        if get_fitness(best) > best_fitness:\n",
    "            best_individual = best\n",
    "            best_fitness = get_fitness(best)\n",
    "\n",
    "    return best_individual        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cga()\n",
    "cga_values = fitness_values[:]\n",
    "\n",
    "ga()\n",
    "ga_values = fitness_values[:]\n",
    "\n",
    "ma()\n",
    "ma_values = fitness_values[:]\n",
    "\n",
    "randomsearch()\n",
    "random_values = fitness_values[:]\n",
    "\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.plot(ma_values, label = \"MA\")\n",
    "plt.plot(random_values, label = \"Random\")\n",
    "plt.plot(cga_values, label = \"CGA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Population-based incremental learning (PBIL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "PBIL is an alternative incremental EDA strategy. It is based on the same probabilistic model, but instead of updating the probabilities based on the difference between the best and worst sampled solution, the probabilities are updated based on the difference between the best sample and the _current_ distribution. That is, the main difference lies in the update function. Given the best individual $x$ from the current sample:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p_{t+1}(i) = p_t(i) + \\lambda \\cdot (x_i - p_t(i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If the best individual $x$ contains a $1$ at bit $i$, then the probability is updated (unless it is already 1.0) based on $x_i - p_t(i)$. If $x$ contains a $0$, then the difference will be negative, thus decreasing the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pbil():\n",
    "    fitness_values.clear()\n",
    "    distribution = [0.5 for _ in range(n)]\n",
    "    best_fitness = -sys.maxsize\n",
    "    best_individual = None\n",
    "    while len(fitness_values) < max_steps:\n",
    "        candidates = [sample(distribution) for _ in range(configuration[\"num_samples\"])]\n",
    "        best = max(candidates, key=lambda k: get_fitness(k))\n",
    "\n",
    "        for i in range(n):\n",
    "            distribution[i] = distribution[i] + configuration[\"lambda\"] * (best[i] - distribution[i])\n",
    "\n",
    "        if get_fitness(best) > best_fitness:\n",
    "            best_individual = best\n",
    "            best_fitness = get_fitness(best)\n",
    "\n",
    "    return best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pbil()\n",
    "pbil_values = fitness_values[:]\n",
    "\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.plot(ma_values, label = \"MA\")\n",
    "plt.plot(random_values, label = \"Random\")\n",
    "plt.plot(cga_values, label = \"CGA\")\n",
    "plt.plot(pbil_values, label = \"PBIL\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Univariate Marginal Distribution Algorithm (UMDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final variant of an EDA we consider, the Univariate Marginal Distribution Algorithm (UMDA), is a population-based EDA. Like in a regular GA we maintain a population of solutions. For each iteration, we select a population of promising solutions using regular selection operators. Then, we compute a probability vector on this selected population of promising solutions, and generate the next generation by sampling this probability vector. This newly sampled population replaces the old one. \n",
    "\n",
    "We will use a simple _truncation selection_ operator that selects the top 50% of the population. The probability for bit $i$ is estimated as the number of $1$s that occur at bit $i$ out of the selected individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cap(value, n):\n",
    "    return max(min(value, 1 - 1/n), 1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def umda():\n",
    "    sample_size = configuration[\"population_size\"]//2\n",
    "    fitness_values.clear()\n",
    "    population = [get_random_solution() for _ in range(configuration[\"population_size\"])]\n",
    "    population.sort(key=lambda k: get_fitness(k), reverse=True)\n",
    "    best_individual = population[0]\n",
    "    best_fitness = get_fitness(best_individual)\n",
    "    \n",
    "    while len(fitness_values) < max_steps:\n",
    "        subset = population[:sample_size]\n",
    "        distribution = []\n",
    "        for i in range(n):\n",
    "            values = [x[i] for x in subset]\n",
    "            distribution.append(cap((sum(values)) / (len(values)), n))\n",
    "\n",
    "        population = [sample(distribution) for _ in range(configuration[\"population_size\"])]\n",
    "        population.sort(key=lambda k: get_fitness(k), reverse=True)\n",
    "\n",
    "        candidate = population[0]\n",
    "        fitness = get_fitness(candidate)\n",
    "        if fitness > best_fitness:\n",
    "            best_fitness = fitness\n",
    "            best_individual = candidate\n",
    "\n",
    "    return best_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is one import aspect when calculating probability estimates for each bit: If we happen to select a population where all instances have that bit set to 0, then the probability of sampling that bit will become 0. Thus, all future generations would definitely take the value 0, and it would be impossible for the algorithm to ever sample a 1 again at that position. To avoid this from happening, we apply Laplace correction in the probability update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "umda()\n",
    "umda_values = fitness_values[:]\n",
    "\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.plot(ma_values, label = \"MA\")\n",
    "plt.plot(random_values, label = \"Random\")\n",
    "plt.plot(cga_values, label = \"CGA\")\n",
    "plt.plot(pbil_values, label = \"PBIL\")\n",
    "plt.plot(umda_values, label = \"UMDA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's look at the performance of the EDA algorithms compared to some other baseline algorithms for a slightly more challenging value of `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#n = 500\n",
    "#results = {\n",
    "#    \"Random\"       : run_times(randomsearch, 10),\n",
    "#    \"Standard GA\"  : run_times(ga, 10),\n",
    "#    \"MA\"           : run_times(ma, 10),\n",
    "#    \"CGA\"          : run_times(cga, 10),\n",
    "#    \"PBIL\"         : run_times(pbil, 10),\n",
    "#    \"UMDA\"         : run_times(umda, 10)\n",
    "#}\n",
    "#fig, ax = plt.subplots(figsize=(12,4))\n",
    "#ax.boxplot(results.values())\n",
    "#ax.set_xticklabels(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_fitness(solution):\n",
    "    if hasattr(solution, \"fitness\"):\n",
    "        return solution.fitness\n",
    "    \n",
    "    fitness = len(solution) - sum(solution)\n",
    "    solution.fitness = fitness\n",
    "    \n",
    "    if not fitness_values:\n",
    "        fitness_values.append(fitness)\n",
    "    else:\n",
    "        fitness_values.append(max(fitness, fitness_values[-1]))\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "umda()\n",
    "umda_values = fitness_values[:]\n",
    "\n",
    "cga()\n",
    "cga_values = fitness_values[:]\n",
    "\n",
    "pbil()\n",
    "pbil_values = fitness_values[:]\n",
    "\n",
    "ga()\n",
    "ga_values = fitness_values[:]\n",
    "\n",
    "ma()\n",
    "ma_values = fitness_values[:]\n",
    "\n",
    "randomsearch()\n",
    "random_values = fitness_values[:]\n",
    "\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.plot(ma_values, label = \"MA\")\n",
    "plt.plot(random_values, label = \"Random\")\n",
    "plt.plot(cga_values, label = \"CGA\")\n",
    "plt.plot(pbil_values, label = \"PBIL\")\n",
    "plt.plot(umda_values, label = \"UMDA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_fitness = hurdle_fitness\n",
    "\n",
    "umda()\n",
    "umda_values = fitness_values[:]\n",
    "\n",
    "cga()\n",
    "cga_values = fitness_values[:]\n",
    "\n",
    "pbil()\n",
    "pbil_values = fitness_values[:]\n",
    "\n",
    "ga()\n",
    "ga_values = fitness_values[:]\n",
    "\n",
    "ma()\n",
    "ma_values = fitness_values[:]\n",
    "\n",
    "randomsearch()\n",
    "random_values = fitness_values[:]\n",
    "\n",
    "plt.plot(ga_values, label = \"GA\")\n",
    "plt.plot(ma_values, label = \"MA\")\n",
    "plt.plot(random_values, label = \"Random\")\n",
    "plt.plot(cga_values, label = \"CGA\")\n",
    "plt.plot(pbil_values, label = \"PBIL\")\n",
    "plt.plot(umda_values, label = \"UMDA\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differential Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Differential evolution is a popular evolutionary algorithm in the context of nonlinear and non-differntiable continuous space functions. The algorithm maintains a population of candidate solutions that move around in the search space. In each iteration, the position of each individual of that population is updated using mathematical formulas; if the update does not lead to an improvement, the individual remains where it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Unlike regular evolutionary algorithms, the reproduction requires _four_ individuals. Assuming we have a candidate target individual $x_t$, then we select three further individuals $x_1$, $x_2$, and $x_3$. First, we create a mutant $m$ from $x_1$, $x_2$, and $x_3$:\n",
    "\n",
    "$$\n",
    "m = x_1 + F \\cdot (x_2 - x_3)\n",
    "$$\n",
    "\n",
    "Here, $F$ is a real number controlling the rate at which the population evolves (e.g., 0.1).\n",
    "\n",
    "Given the mutant $m$ and the target individual $x_t$, we now create a new individual using uniform crossover between $m$ and $x_t$, but with a twist: At one randomly chosen position the child allele is taken from the first parent without making a random decision (duplication of second parent not possible).\n",
    "\n",
    "Finally, deterministic selection is applied to decide whether the resulting offspring should replace $x_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While Differential Evolution was originally designed for real-valued problems, it is possible to adapt it to bitstring representations. When creating the mutant $m$, for each bit $i$ we set $m(i)$ to $1 - x_1(i)$ if $x_2(i) \\neq x_3(i)$ and with probability $F$, and to $x_1(i)$ otherwise. That is, the individuals $x_2$ and $x_3$, together with a random number, decide whether each bit in $x_1$ is flipped or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def binaryde():\n",
    "    F = 0.1\n",
    "    fitness_values.clear()\n",
    "    population = [get_random_solution() for _ in range(configuration[\"population_size\"])]\n",
    "\n",
    "    while len(fitness_values) < max_steps:\n",
    "        for j in range(len(population)):\n",
    "\n",
    "            # select three random vector index positions [0, popsize), not including current vector (j)\n",
    "            candidates = [i for i in range(len(population))]\n",
    "            candidates.remove(j)\n",
    "            random_index = random.sample(candidates, 3)\n",
    "\n",
    "            x_1 = population[random_index[0]]\n",
    "            x_2 = population[random_index[1]]\n",
    "            x_3 = population[random_index[2]]\n",
    "            x_t = population[j]  # target individual\n",
    "\n",
    "            # Mutation\n",
    "            mutant = L([])\n",
    "            for k in range(len(x_t)):\n",
    "                if x_2[k] != x_3[k] and random.random() < F:\n",
    "                    mutant.append(1 - x_1[k])\n",
    "                else:\n",
    "                    mutant.append(x_1[k])\n",
    "\n",
    "            # Uniform crossover\n",
    "            trial = L([])\n",
    "            for k in range(len(x_t)):\n",
    "                if random.random() <= 0.5:\n",
    "                    trial.append(mutant[k])\n",
    "                else:\n",
    "                    trial.append(x_t[k])\n",
    "\n",
    "            # Greedy selection\n",
    "            if get_fitness(trial) >= get_fitness(x_t):\n",
    "                population[j] = trial\n",
    "\n",
    "\n",
    "    return max(population, key=lambda k: get_fitness(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's consider the standard one-max problem again for this comparison, as we have more fine grained fitness values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "get_fitness = onemax_fitness\n",
    "n = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "binaryde()\n",
    "values_de = fitness_values[:]\n",
    "\n",
    "ga()\n",
    "values_ga = fitness_values[:]\n",
    "\n",
    "plt.plot(values_ga, label = \"GA\")\n",
    "plt.plot(values_de, label = \"DE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Very likely, the differential evolution gets stuck quite soon; this is an effect of the small population sizes -- differential evolution tends to perform badly with population sizes of less than 50. Let's try again with a larger population size. For simplicity, we use the same population size for other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "configuration[\"population_size\"] = 100\n",
    "binaryde()\n",
    "values_de = fitness_values[:]\n",
    "\n",
    "ga()\n",
    "values_ga = fitness_values[:]\n",
    "\n",
    "plt.plot(values_ga, label = \"GA\")\n",
    "plt.plot(values_de, label = \"DE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyper Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since we now have several different heuristics to choose from, we could implement a hyper-heuristic to choose the best one for the problem at hand. For this, we need a function that probabilistically chooses a heuristic based on its recent performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def select(heuristics, history):\n",
    "\n",
    "    w = 5\n",
    "    weights = []\n",
    "    for h in heuristics:\n",
    "        weights.append(max(history[h][-w:]))\n",
    "\n",
    "    candidates = random.sample([x for x in range(len(weights))], 3)\n",
    "    index = max(candidates, key=lambda k: weights[k])\n",
    "    return heuristics[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We also need to slightly adapt our heuristics: We want to apply each heuristic only for a couple of iterations, and then let the hyper heuristic choose how to proceed. This means we require two parameters: The population which serves as starting point for the current invocation of the heuristic, and the number of iterations to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hyper_ga(population, iterations):\n",
    "    best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_solution)\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < iterations:\n",
    "        iteration += 1\n",
    "        new_population = []\n",
    "\n",
    "        while len(new_population) < len(population):\n",
    "            parent1 = tournament_selection(population)\n",
    "            parent2 = tournament_selection(population)\n",
    "\n",
    "            if random.random() < configuration[\"P_xover\"]:\n",
    "                offspring1, offspring2 = crossover(parent1, parent2)\n",
    "            else:\n",
    "                offspring1, offspring2 = parent1, parent2\n",
    "\n",
    "            offspring1 = mutate(offspring1)\n",
    "            offspring2 = mutate(offspring2)\n",
    "\n",
    "            new_population.append(offspring1)\n",
    "            new_population.append(offspring2)\n",
    "\n",
    "        population.clear()\n",
    "        population.extend(new_population)\n",
    "\n",
    "        best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "        best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    return best_solution, best_fitness, population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hyper_ma(population, iterations):\n",
    "    best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_solution)\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < iterations:\n",
    "        iteration += 1\n",
    "        new_population = []\n",
    "\n",
    "        while len(new_population) < len(population):\n",
    "            parent1 = tournament_selection(population)\n",
    "            parent2 = tournament_selection(population)\n",
    "\n",
    "            if random.random() < configuration[\"P_xover\"]:\n",
    "                offspring1, offspring2 = crossover(parent1, parent2)\n",
    "            else:\n",
    "                offspring1, offspring2 = parent1, parent2\n",
    "\n",
    "            offspring1 = mutate(offspring1)\n",
    "            offspring2 = mutate(offspring2)\n",
    "\n",
    "            if random.random() < configuration[\"P_localsearch\"]:\n",
    "                offspring1, offspring1_fitness = hillclimbing(offspring1)\n",
    "                offspring2, offspring2_fitness = hillclimbing(offspring2)\n",
    "\n",
    "            new_population.append(offspring1)\n",
    "            new_population.append(offspring2)\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "        best_solution = max(population, key=lambda k: get_fitness(k))\n",
    "        best_fitness = get_fitness(best_solution)\n",
    "\n",
    "    return best_solution, best_fitness, population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hyper_binaryde(population, iterations):\n",
    "    F = 0.1\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < iterations:\n",
    "        iteration += 1\n",
    "        for j in range(len(population)):\n",
    "\n",
    "            # select three random vector index positions [0, popsize), not including current vector (j)\n",
    "            candidates = [i for i in range(len(population))]\n",
    "            candidates.remove(j)\n",
    "            random_index = random.sample(candidates, 3)\n",
    "\n",
    "            x_1 = population[random_index[0]]\n",
    "            x_2 = population[random_index[1]]\n",
    "            x_3 = population[random_index[2]]\n",
    "            x_t = population[j]  # target individual\n",
    "\n",
    "            # Mutation\n",
    "            mutant = L([])\n",
    "            for k in range(len(x_t)):\n",
    "                if x_2[k] != x_3[k] and random.random() < F:\n",
    "                    mutant.append(1 - x_1[k])\n",
    "                else:\n",
    "                    mutant.append(x_1[k])\n",
    "\n",
    "            # Binomial crossover\n",
    "            trial = L([])\n",
    "            for k in range(len(x_t)):\n",
    "                if random.random() <= 0.5: # uniform xover\n",
    "                    trial.append(mutant[k])\n",
    "                else:\n",
    "                    trial.append(x_t[k])\n",
    "\n",
    "            # Greedy selection\n",
    "            if get_fitness(trial) >= get_fitness(x_t):\n",
    "                population[j] = trial\n",
    "\n",
    "    best_individual = max(population, key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_individual)\n",
    "\n",
    "    return best_individual, best_fitness, population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We could adapt further heuristics, and also allow further choices regarding the different qualitative and quantitative parameters. However, we'll keep it simple; the hyper-heuristic now simply runs each heuristic in turn for a number of iterations (e.g., 5) and keeps track of the improvement achieved at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hyper():\n",
    "    fitness_values.clear()\n",
    "    population =[get_random_solution() for _ in range(configuration[\"population_size\"])]\n",
    "    best_individual = max(population, key=lambda k: get_fitness(k))\n",
    "    best_fitness = get_fitness(best_individual)\n",
    "    step_size = 5\n",
    "    heuristics = [hyper_ga, hyper_ma, hyper_binaryde] \n",
    "\n",
    "    history = {}\n",
    "    for h in heuristics:\n",
    "        # Use high value for heuristics not tried yet, to ensure they are attemped\n",
    "        history[h] = [n]\n",
    "\n",
    "    iteration = 0\n",
    "    while len(fitness_values) < max_steps:\n",
    "        h = select(heuristics, history)\n",
    "        best_individual, next_fitness, population = h(population, step_size)\n",
    "\n",
    "        history[h].append(next_fitness - best_fitness)\n",
    "        best_fitness = next_fitness\n",
    "        iteration += step_size\n",
    "\n",
    "    for h in heuristics:\n",
    "        print(f\"Heuristic {h.__name__}: {len(history[h])} invocations\")\n",
    "        \n",
    "    return best_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our previous experiments have shown that the MA tends to use up substantial numbers of fitness evaluations for large values of `n` and large populations. As discussed above, we should really be limiting the number of fitness evaluations done during the local search, but to make the algorithm perform well on large values of `n` for now, we will simply reduce the probability of applying local search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "configuration[\"P_localsearch\"] = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now let's compare the individual constituent algorithms, and see whether the hyper heuristic is able to figure out the best heuristic for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_fitness = onemax_fitness\n",
    "\n",
    "ga()\n",
    "values_ga = fitness_values[:]\n",
    "\n",
    "ma()\n",
    "values_ma = fitness_values[:]\n",
    "\n",
    "binaryde()\n",
    "values_de = fitness_values[:]\n",
    "\n",
    "hyper()\n",
    "hyper_values = fitness_values[:]\n",
    "\n",
    "plt.plot(values_ga, label = \"GA\")\n",
    "plt.plot(values_ma, label = \"MA\")\n",
    "plt.plot(values_de, label = \"DE\")\n",
    "plt.plot(hyper_values, label = \"Hyper\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's also repeat this for the hurdle fitness function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "get_fitness = hurdle_fitness\n",
    "\n",
    "ga()\n",
    "values_ga = fitness_values[:]\n",
    "\n",
    "ma()\n",
    "values_ma = fitness_values[:]\n",
    "\n",
    "binaryde()\n",
    "values_de = fitness_values[:]\n",
    "\n",
    "hyper()\n",
    "hyper_values = fitness_values[:]\n",
    "\n",
    "plt.plot(values_ga, label = \"GA\")\n",
    "plt.plot(values_ma, label = \"MA\")\n",
    "plt.plot(values_de, label = \"DE\")\n",
    "plt.plot(hyper_values, label = \"Hyper\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
