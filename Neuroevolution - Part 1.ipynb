{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5194253b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neuroevolution\n",
    "\n",
    "Neuroevolution uses evolutionary algorithms to optimise neural networks. Before we start with the implementation of neural networks, let's import required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e9214",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2510c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks consist of neurons and weighted connections between these neurons. Each neuron represents a processing unit in which an activation function is applied to the weighted sum of all incoming connections. After a neuron has been activated, its activation signal is further propagated into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a17fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Basic neuron definition from which all neuron genes will be derived.\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        self.uid = uid\n",
    "        self.signal_value = 0\n",
    "        self.activation_value = 0\n",
    "        self.incoming_connections = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb01471",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input Neuron\n",
    "\n",
    "Input neurons receive a signal from the environment (input feature) and propagate it into the network. Since we are interested in the raw input signal, we refrain from applying any activation functions within the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502a0ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class InputNeuron(Neuron):\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        super().__init__(uid)\n",
    "\n",
    "    def activate(self) -> float:\n",
    "        self.activation_value = self.signal_value\n",
    "        return self.activation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc784b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Neuron\n",
    "\n",
    "Hidden neurons reside between input and output neurons. They increase the capacity of a neural network, in other words, we require more hidden neurons for complex tasks than for simple ones. The number and distribution of neurons is an optimisation task on its own and one of the main reasons to use neuroevolution. As an activation function, we will use the sigmoid function, which maps any input value to $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2359a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: float):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "x_range = np.arange(-10, 10, 0.01)\n",
    "sigmoid_values = [sigmoid(x) for x in x_range]\n",
    "plt.plot(x_range, sigmoid_values)\n",
    "plt.title('Sigmoid function', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb70e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenNeuron(Neuron):\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        super().__init__(uid)\n",
    "\n",
    "    def activate(self) -> float:\n",
    "        self.activation_value = sigmoid(self.signal_value)\n",
    "        return self.activation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bd3c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output Neuron\n",
    "\n",
    "For our output neurons, we have to choose an appropriate activation function that matches the given task. Later, we will try to solve a binary decision problem in which a robot can move to the right or left. Hence, we could use a sigmoid function, whose output is treated as a probability for choosing one of the two classes. However, due to the neuroevolution algorithm we will implement, we add two output neurons to the output layer, assign one output neuron to each class, and choose the class whose output neuron has the highest activation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360003fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class OutputNeuron(Neuron):\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        super().__init__(uid)\n",
    "        self.uid = uid\n",
    "\n",
    "    def activate(self) -> float:\n",
    "        self.activation_value = self.signal_value\n",
    "        return self.activation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1468034",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connections\n",
    "\n",
    "Our neurons are rather useless as long as they are not connected. We define connections by their source and destination neurons. Furthermore, we assign a weight to each connection to specify the strength of a link between two neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67348a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Connection:\n",
    "\n",
    "    def __init__(self, source: Neuron, target: Neuron, weight: float):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.weight = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b5e0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Network Definition\n",
    "\n",
    "With our neurons and connections defined, we can now assemble both components in layers to build a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380ca0c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers: dict[float, list[Neuron]], connections: list[Connection]):\n",
    "        self.layers = layers\n",
    "        self.connections = connections\n",
    "        self.generate()\n",
    "\n",
    "    # Traverse list of connections and add incoming connections to neurons.\n",
    "    def generate(self):\n",
    "        for connection in self.connections:\n",
    "            connection.target.incoming_connections.append(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3c500",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's build a network with five input neurons, three hidden neurons and two output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4a1c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gen_simple_network() -> Network:\n",
    "    input_neurons = [InputNeuron(f\"I{x}\") for x in range(5)]\n",
    "    hidden_neurons = [HiddenNeuron(f\"H{x}\") for x in range(3)]\n",
    "    output_neurons = [OutputNeuron(f\"O{x}\") for x in range(5)]\n",
    "\n",
    "    # Assemble in layers with 0 representing the input layer and 1 the output layer.\n",
    "    layers = {0: input_neurons, 0.5: hidden_neurons, 1: output_neurons}\n",
    "\n",
    "    # Generate connections from every input neuron to each hidden neuron\n",
    "    connections = []\n",
    "    for input_neuron in input_neurons:\n",
    "        for hidden_neuron in hidden_neurons:\n",
    "            weight = random.uniform(-1, 1)\n",
    "            connections.append(Connection(input_neuron, hidden_neuron, weight))\n",
    "\n",
    "    # Generate connections from every hidden neuron to each output neuron\n",
    "    for hidden_neuron in hidden_neurons:\n",
    "        for output_neuron in output_neurons:\n",
    "            weight = random.uniform(-1, 1)\n",
    "            connections.append(Connection(hidden_neuron, output_neuron, weight))\n",
    "\n",
    "    return Network(layers, connections)\n",
    "\n",
    "\n",
    "net = gen_simple_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578347a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can visualise our generated network using the Graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99114fdc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Network(Network):\n",
    "    def show(self) -> Digraph:\n",
    "        dot = Digraph(graph_attr={'rankdir': 'BT', 'splines': \"line\"})\n",
    "        # Use sub graphs to position input neurons are at the bottom and output neurons at the top.\n",
    "        input_graph = Digraph(graph_attr={'rank': 'min', 'splines': \"line\"})\n",
    "        output_graph = Digraph(graph_attr={'rank': 'max', 'splines': \"line\"})\n",
    "        hidden_graph = Digraph(graph_attr={'splines': \"line\"})\n",
    "\n",
    "        # Traverse network from input to output layer and assign neurons to corresponding subgraph.\n",
    "        layer_keys = list(self.layers.keys())\n",
    "        layer_keys.sort()\n",
    "        for layer in layer_keys:\n",
    "            for neuron in self.layers.get(layer):\n",
    "                if layer == 0:\n",
    "                    input_graph.node(neuron.uid, color='black', fillcolor='white', style='filled')\n",
    "                elif layer == 1:\n",
    "                    output_graph.node(neuron.uid, color='black', fillcolor='white', style='filled')\n",
    "                else:\n",
    "                    hidden_graph.node(neuron.uid, color='black', fillcolor='white', style='filled')\n",
    "\n",
    "        # Combine the sub graphs to a single graph\n",
    "        dot.subgraph(input_graph)\n",
    "        dot.subgraph(hidden_graph)\n",
    "        dot.subgraph(output_graph)\n",
    "        # Link the nodes based on the connection gene.\n",
    "        for connection in self.connections:\n",
    "            dot.edge(connection.source.uid, connection.target.uid,\n",
    "                     label=str(round(connection.weight, 2)), style='solid')\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1397180",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gen_simple_network().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8dec9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have our network structure in place, we can implement the network activation that takes an input signal as an argument and outputs the result of the network after it has been activated. We can realise a network's activation by computing our neurons' activation values sequentially from the input to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee32918",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Network(Network):\n",
    "    def activate(self, inputs: list[float]) -> list[float]:\n",
    "        # Reset neuron values from previous executions\n",
    "        for neuron_layer in self.layers.values():\n",
    "            for neuron in neuron_layer:\n",
    "                neuron.signal_value = 0\n",
    "\n",
    "        # Load input features into input neurons\n",
    "        input_neurons = self.layers.get(0)\n",
    "        for i in range(len(inputs)):\n",
    "            input_neurons[i].signal_value = inputs[i]\n",
    "            input_neurons[i].activate()\n",
    "\n",
    "        # Traverse the neurons of our network sequentially starting from the hidden layer.\n",
    "        layer_keys = list(self.layers.keys())\n",
    "        layer_keys.sort()\n",
    "        for layer in layer_keys:\n",
    "            for neuron in self.layers.get(layer):\n",
    "                if layer > 0:\n",
    "                    # Calculate weighted sum of incoming connections\n",
    "                    for connection in neuron.incoming_connections:\n",
    "                        neuron.signal_value += connection.source.activation_value * connection.weight\n",
    "                neuron.activate()\n",
    "\n",
    "        output_neurons = self.layers.get(1)\n",
    "        return [o.activation_value for o in output_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84c0fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gen_simple_network().activate([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8b03a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverted Pendulum Problem\n",
    "\n",
    "We will use our networks to solve the inverted pendulum (= pole-balancing) problem, a well-known benchmark task in the reinforcement learning community. In this problem scenario, a pole is centred on a cart that can be moved to the right and left. Obviously, any movement to the cart also impacts the pole. The task is to move the cart to the left and right such that the pole remains balanced. Whenever the cart position exceeds the boundaries of the track or the pole tips over 12 degrees, the balancing attempt is deemed a failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f109a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can simulate the cart and pole system using the following two equations that describe the acceleration of the pole $\\ddot{\\theta_t}$ and the cart $\\ddot{p_t}$ at a given point in time $t$:\n",
    "\n",
    "$$\n",
    "\\ddot{p}_t = \\frac{F_t + m_pl(\\dot{\\theta^2_t} sin\\theta_t - \\ddot{\\theta_t} \\; cos\\theta_t)}{m} \\; \\; \\; \\; \\; \\; \\; \\; (1) \\\\\n",
    "\\ddot{\\theta}_t = \\frac{mg \\; sin\\theta_t - cos\\theta_t(F_t + m_pl\\dot{\\theta}^2_t \\; sin\\theta_t)}{\\frac{4}{3}ml - m_pl \\; cos^2(\\theta_t)} \\; \\; \\; \\; \\; \\; \\; \\; (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d171e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "where\n",
    "- $p$: Position of the cart\n",
    "- $\\dot{p}$: Velocity of the cart\n",
    "- $\\ddot{p}$: Acceleration of the cart\n",
    "- $\\theta$: Angle of the pole\n",
    "- $\\dot{\\theta}$: Angular velocity of the pole\n",
    "- $\\ddot{\\theta}$: Angular acceleration of the pole\n",
    "- $l$: Length of the pole\n",
    "- $m_p$: Mass of the pole\n",
    "- $m$: Mass of the pole and cart\n",
    "- $F$: Force applied to the cart\n",
    "- $g$: Gravity acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb47888",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our simulation, we will update both systems using the discrete-time equations\n",
    "\n",
    "$$\n",
    "p(t+1) = p(t) + r \\dot{p}(t) \\; (3) \\\\\n",
    "\\theta(t+1) = \\theta(t) + r \\dot{\\theta}(t) \\;(4) \\\\\n",
    "\\dot{p}(t+1) = \\dot{p}(t) + r \\ddot{p}(t) \\; (5) \\\\\n",
    "\\dot{\\theta}(t+1) = \\dot{\\theta}(t) + r \\ddot{\\theta}(t) \\; (6)\n",
    "$$\n",
    "\n",
    "with the discrete time step $r$ set to 0.02 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07975ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "GRAVITY = 9.8  # m/s^2\n",
    "MASS_CART = 1.0  # kg\n",
    "MASS_POLE = 0.1  # kg\n",
    "TOTAL_MASS = (MASS_CART + MASS_POLE)  # kg\n",
    "POLE_LENGTH = 0.5  # m\n",
    "POLEMASS_LENGTH = (MASS_POLE * POLE_LENGTH)\n",
    "FORCE = 10  # N\n",
    "STEP_SIZE = 0.02  # sec\n",
    "FOURTHIRDS = 1.333333\n",
    "\n",
    "\n",
    "def cart_pole_step(action: int, p: float, p_vel: float, theta: float, theta_vel: float) -> (float, float, float, float):\n",
    "    force_dir = FORCE if action > 0 else -FORCE\n",
    "\n",
    "    cos_theta = math.cos(theta)\n",
    "    sin_theta = math.sin(theta)\n",
    "\n",
    "    temp = (force_dir + POLEMASS_LENGTH * theta_vel * theta_vel * sin_theta) / TOTAL_MASS\n",
    "\n",
    "    # Equation 2\n",
    "    theta_acc = (GRAVITY * sin_theta - cos_theta * temp) / (\n",
    "                POLE_LENGTH * (FOURTHIRDS - MASS_POLE * cos_theta * cos_theta / TOTAL_MASS))\n",
    "    # Equation 1\n",
    "    p_acc = temp - POLEMASS_LENGTH * theta_acc * cos_theta / TOTAL_MASS\n",
    "\n",
    "    # Compute new states\n",
    "    p = p + STEP_SIZE * p_vel  # Equation 3\n",
    "    theta = theta + STEP_SIZE * theta_vel  # Equation 4\n",
    "    p_vel = p_vel + STEP_SIZE * p_acc  # Equation 5\n",
    "    theta_vel = theta_vel + STEP_SIZE * theta_acc  # Equation 6\n",
    "\n",
    "    return p, p_vel, theta, theta_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36c001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test our cart pole system by executing a few steps starting from a clean state where all state variables are set to zero. Given a balanced pole ($\\theta = 0$), we expect the pole to always move in the opposite direction of the applied force. Moreover, we expect increasing velocity values as long as we apply a force in the same direction and a decrease in velocity as soon as the force direction changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f56bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = 0\n",
    "p_vel = 0\n",
    "theta = 0\n",
    "theta_vel = 0\n",
    "# Move the cart for 10 steps to the right.\n",
    "for i in range(10):\n",
    "    [p, p_vel, theta, theta_vel] = cart_pole_step(1, p, p_vel, theta, theta_vel)\n",
    "    print(f\"Iteration right: {i}\\nCart Pos: {p}\\nCart Vel: {p_vel}\\nPole Angle: {theta}\\nPole AVel: {theta_vel}\")\n",
    "    print(\"--------------------\")\n",
    "          \n",
    "# Move the cart for 15 steps to the left.\n",
    "for i in range(15):\n",
    "    [p, p_vel, theta, theta_vel] = cart_pole_step(-1, p, p_vel, theta, theta_vel)\n",
    "    print(f\"Iteration left: {i}\\nCart Pos: {p}\\nCart Vel: {p_vel}\\nPole Angle: {theta}\\nPole AVel: {theta_vel}\")\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c21f5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can formulate our pole-balancing task as a reinforcement learning problem by implementing a simulation in which an agent's goal is to balance the pole for as long as possible. In our scenario, our agents are neural networks that decide in which direction the cart should be moved at a given state $[p, \\theta, \\dot{p}, \\dot{\\theta}]$. Since we have four input features, the networks will have four input neurons. Furthermore, these input features will be normalised over the following ranges:\n",
    "\n",
    "$$\n",
    "p: [-2.4, 2.4] \\\\\n",
    "\\dot{p}: [-1.5, 1.5] \\\\\n",
    "\\theta: [-12, 12] \\\\\n",
    "\\dot{\\theta}: [-60, 60] \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d478b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We model the pole-balancing problem as a binary classification task using two output neurons, each representing one action. An action is chosen by selecting the action that belongs to the neuron with the highest activation value. Once the pole tips over 12 degrees or the 4.8 meter track is left by the cart, the trial ends. Finally, an agent's performance is measured by the number of steps it managed to survive. A balancing attempt is deemed successful whenever an agent balances the pole for 120,000 time steps, which is equivalent to 40 minutes in real time.\n",
    "\n",
    "For a more challenging second scenario, we add an option that randomises the starting positions of the four input states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0a4d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 120000\n",
    "TWELVE_DEGREES = 0.2094395  #conversion to rad (12*pi)/180\n",
    "\n",
    "def evaluate_agent(network: Network, random_start=False) -> int:\n",
    "    # Define starting state\n",
    "    if random_start:\n",
    "        p = random.uniform(-2.4, 2.4)  # -2.4 < p < 2.4\n",
    "        p_vel = random.uniform(-1.5, 1.5)  # -1.5 < p_acc < 1.5\n",
    "        theta = random.uniform(-TWELVE_DEGREES, TWELVE_DEGREES)  # -12 < theta < 12\n",
    "        theta_vel = random.uniform(-1, 1)  # -60 < theta_acc < 60 (in rad)\n",
    "    else:\n",
    "        p, p_vel, theta, theta_vel = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # Simulation loop\n",
    "    steps = 0\n",
    "    while steps < MAX_STEPS:\n",
    "        # Normalise inputs\n",
    "        inputs = [None] * 5\n",
    "        inputs[0] = (p + 2.4) / 4.8\n",
    "        inputs[1] = (p_vel + 1.5) / 3\n",
    "        inputs[2] = (theta + TWELVE_DEGREES) / (2 * TWELVE_DEGREES)\n",
    "        inputs[3] = (theta_vel + 1.0) / 2.0\n",
    "        inputs[4] = 0.5\n",
    "\n",
    "        # Activate the network and interpret the output, and advance the state.\n",
    "        net_output = network.activate(inputs)\n",
    "        action = -1 if net_output[0] > net_output[1] else 1\n",
    "        p, p_vel, theta, theta_vel = cart_pole_step(action, p, p_vel, theta, theta_vel)\n",
    "\n",
    "        # Check if the attempt is still valid\n",
    "        if p < -2.4 or p > 2.4 or theta < -TWELVE_DEGREES or theta > TWELVE_DEGREES:\n",
    "            return steps\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    # At this point the agent survived for the maximum number of steps\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a693e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a sample network and test it in the pole-balancing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bda096",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = gen_simple_network()\n",
    "evaluate_agent(net, random_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d7e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As expected, our randomly generated networks could perform better. Therefore, we will now implement a Neuroevolution algorithm to optimise our networks and solve the pole-balancing task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15397f91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SANE (Symbiotic Adaptive Neuro-Evolution)\n",
    "\n",
    "[SANE](https://link.springer.com/article/10.1023/A:1018004120707) implements a symbiotic evolution approach, which means that it does not evolve complete solutions but parts thereof. In the case of SANE, we maintain a population of hidden neurons, which are then combined to form a neural network based on a pre-defined input and output layer. Symbiotic approaches aim to avoid premature convergence by enforcing diverse populations and specialisation of individuals in specific subtasks within the problem environment. Each individual receives a fitness score based on the average fitness of the networks in which they participated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712bfca1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Genetic Representation\n",
    "\n",
    "Each neuron is defined by a label and weight list. The former defines the neuron from/to which the connection is coming/going, and the latter the corresponding link weight. We follow the original SANE implementation of Moriarty and Mikkulainen and define an upper bound of 254 for our label values. We connect from the input layer to the hidden neuron if the label value is smaller than 127, and otherwise, from the hidden neuron to the output layer. Moreover, we apply modulo calculus over the total number of input/output neurons to decide to which specific input/output neuron a connection is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018251e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "LABEL_BOUND = 254\n",
    "\n",
    "\n",
    "class HiddenNeuronGene:\n",
    "    hidden_neuron_counter = 0\n",
    "\n",
    "    def __init__(self, labels: list[int], weights: list[float]):\n",
    "        self.labels = [l % LABEL_BOUND for l in labels]  # Enforce upper label boundary.\n",
    "        self.weights = weights\n",
    "        self.fitness = 0\n",
    "        self.evaluations = 0\n",
    "        self.hidden_neuron = HiddenNeuron(\"H\" + str(HiddenNeuronGene.hidden_neuron_counter))\n",
    "        HiddenNeuronGene.hidden_neuron_counter += 1\n",
    "\n",
    "    def generate_connections(self, input_neurons: list[Neuron], output_neurons: list[Neuron]) -> list[Connection]:\n",
    "        connections = []\n",
    "        for label, weight in zip(self.labels, self.weights):\n",
    "\n",
    "            # Generate connection to input layer\n",
    "            if label < 127:\n",
    "                neuron_id = \"I\" + str(label % len(input_neurons))\n",
    "                in_neuron = next((i for i in input_neurons if i.uid == neuron_id), None)\n",
    "                connections.append(Connection(in_neuron, self.hidden_neuron, weight))\n",
    "\n",
    "            # Generate connection to output layer\n",
    "            else:\n",
    "                neuron_id = \"O\" + str(label % len(output_neurons))\n",
    "                out_neuron = next((o for o in output_neurons if o.uid == neuron_id), None)\n",
    "                connections.append(Connection(self.hidden_neuron, out_neuron, weight))\n",
    "\n",
    "        return connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17657c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test our gene implementation by generating a neural network using three randomly generated HiddenNeuronGenes, each realising five connections. Moreover, we implement a helper function to create chromosomes with random gene values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b45b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def generate_random_gene_sane(num_connections: int) -> HiddenNeuronGene:\n",
    "    labels = []\n",
    "    weights = []\n",
    "    for _ in range(num_connections):\n",
    "        labels.append(math.floor(random.uniform(0, LABEL_BOUND)))\n",
    "        weights.append(random.uniform(-1, 1))\n",
    "    return HiddenNeuronGene(labels, weights)\n",
    "\n",
    "\n",
    "def generate_network_SANE(chromosomes: list[HiddenNeuronGene]) -> Network:\n",
    "    input_neurons = [InputNeuron(f\"I{x}\") for x in range(5)]\n",
    "    output_neurons = [OutputNeuron(f\"O{x}\") for x in range(2)]\n",
    "\n",
    "    # Extract the hidden neurons and their connections from the provided chromosomes.\n",
    "    hidden_neurons = []\n",
    "    connections = []\n",
    "    for chromosome in chromosomes:\n",
    "        hidden_neurons.append(chromosome.hidden_neuron)\n",
    "        connections.extend(chromosome.generate_connections(input_neurons, output_neurons))\n",
    "\n",
    "    # Assemble in layers.\n",
    "    layers = {0: input_neurons, 0.5: hidden_neurons, 1: output_neurons}\n",
    "\n",
    "    return Network(layers, connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37387e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "genes = [generate_random_gene_sane(5) for x in range(3)]\n",
    "net = generate_network_SANE(genes)\n",
    "net.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9b1b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Crossover\n",
    "\n",
    "SANE primarily breeds offspring using a single-point crossover operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d31f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crossover_sane(parent_1: HiddenNeuronGene, parent_2: HiddenNeuronGene) -> [HiddenNeuronGene, HiddenNeuronGene]:\n",
    "    # Extract the genes from both parents\n",
    "    label_gene_1, weight_gene_1 = parent_1.labels, parent_1.weights\n",
    "    label_gene_2, weight_gene_2 = parent_2.labels, parent_2.weights\n",
    "\n",
    "    # Determine the crossover point. We assume both parent genes have the same gene lengths.\n",
    "    crossover_point = math.floor(random.uniform(1, len(label_gene_1)))\n",
    "\n",
    "    # Apply the crossover operation\n",
    "    child_1_labels = label_gene_1[:crossover_point] + label_gene_2[crossover_point:]\n",
    "    child_1_weights = weight_gene_1[:crossover_point] + weight_gene_2[crossover_point:]\n",
    "    child_2_labels = label_gene_2[:crossover_point] + label_gene_1[crossover_point:]\n",
    "    child_2_weights = weight_gene_2[:crossover_point] + weight_gene_1[crossover_point:]\n",
    "\n",
    "    # Breed and return the two children.\n",
    "    return [HiddenNeuronGene(child_1_labels, child_1_weights), HiddenNeuronGene(child_2_labels, child_2_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f0b87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "parent_1 = generate_random_gene_sane(5)\n",
    "parent_2 = generate_random_gene_sane(5)\n",
    "parent_1.weights = [round(w * 100) / 100 for w in parent_1.weights]  # Round weights for better visualisation\n",
    "parent_2.weights = [round(w * 100) / 100 for w in parent_2.weights]\n",
    "[child_1, child_2] = crossover_sane(parent_1, parent_2)\n",
    "\n",
    "print(\"Neuron \\t\\t| Labels \\t\\t\\t| Weights\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(f\"Parent 1 \\t| {parent_1.labels} \\t| {parent_1.weights}\")\n",
    "print(f\"Parent 2 \\t| {parent_2.labels} \\t| {parent_2.weights}\")\n",
    "print(f\"Child 1 \\t| {child_1.labels} \\t| {child_1.weights}\")\n",
    "print(f\"Child 2 \\t| {child_2.labels} \\t| {child_2.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696ae8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mutation\n",
    "\n",
    "The mutation operator adds gaussian noise to the values of the two gene lists.\n",
    " \n",
    "In contrast, to other evolutionary algorithms, SANE uses mutation only to add genetic material that may be missing from the initial population. Thus, mutation is not responsible for creating diversity within the population as diversity is already present due to the symbiotic evolution approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb49303",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "MUTATION_POWER = 2\n",
    "\n",
    "\n",
    "def mutate_sane(parent: HiddenNeuronGene) -> HiddenNeuronGene:\n",
    "    mutated_labels = []\n",
    "    mutated_weights = []\n",
    "    # Add gaussian noise to the values.\n",
    "    for label, weight in zip(parent.labels, parent.weights):\n",
    "        mutated_labels.append(math.floor(np.random.normal(label, MUTATION_POWER)))\n",
    "        mutated_weights.append(np.random.normal(weight, MUTATION_POWER / 10))\n",
    "\n",
    "    return HiddenNeuronGene(mutated_labels, mutated_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38c061",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "parent = generate_random_gene_sane(5)\n",
    "mutant = mutate_sane(parent)\n",
    "parent.weights = [round(w * 100) / 100 for w in parent.weights]  # Round weights for better visualisation\n",
    "mutant.weights = [round(w * 100) / 100 for w in mutant.weights]\n",
    "print(\"Neuron \\t|\\t\\t Labels \\t\\t|\\t\\t Weights\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(f\"Parent \\t|\\t {parent.labels} \\t|\\t {parent.weights}\")\n",
    "print(f\"Mutant \\t|\\t {mutant.labels} \\t|\\t {mutant.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fc809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evolution\n",
    "\n",
    "A new generation is formed by applying crossover to the best-performing members of the current population. Parents are selected sequentially, and mating partners must have a fitness value at least as high as the selected parent. Finally, we apply mutation to fill the remaining slots in our population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da841231",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUM_PARENTS = 50\n",
    "POPULATION_SIZE = 200\n",
    "\n",
    "\n",
    "def evolve_sane(population: list[HiddenNeuronGene]) -> HiddenNeuronGene:\n",
    "    # Sort the population such that the best-performing chromosomes are at the beginning.\n",
    "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "    offspring = []\n",
    "\n",
    "    # Crossover\n",
    "    for i in range(NUM_PARENTS):\n",
    "        first_parent = population[i]\n",
    "        second_parent = population[math.floor(random.uniform(0, i))]\n",
    "        offspring.extend(crossover_sane(first_parent, second_parent))\n",
    "\n",
    "    # Mutation\n",
    "    while len(offspring) < POPULATION_SIZE:\n",
    "        offspring.append(mutate_sane(random.choice(population)))\n",
    "\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09cbc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "population = [generate_random_gene_sane(5) for i in range(POPULATION_SIZE)]\n",
    "offspring = evolve_sane(population)\n",
    "print(f\"Parent Population Size: {len(population)}\")\n",
    "print(f\"Offspring Population Size: {len(offspring)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f16b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitness evaluation\n",
    "\n",
    "Since our population consists of partial solutions and not full-fledged neural networks, we have to combine several chromosomes into a neural network for each evaluation episode. The fitness of a single chromosome is set to the average fitness of all networks in which it participated. For our pole-balancing task, we combine eight chromosomes, each realising five connections. Hence, we obtain networks that have a total of 40 connections. In each iteration, we generate 200 networks while maintaining a neuron population size of 200, which means that, on average, each neuron is allowed to participate in eight networks per generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb356b74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_NETWORKS, POPULATION_SIZE, NUM_HIDDEN_NEURONS, NUM_CONNECTIONS = 200, 200, 8, 5\n",
    "\n",
    "def evaluate_population_sane(population: list[HiddenNeuronGene], evaluations: int, randomise=False) -> (int, int):\n",
    "    for c in population:     # Reset fitness and evaluation counter for each neuron\n",
    "        c.fitness = 0\n",
    "        c.evaluations = 0\n",
    "\n",
    "    best = 0\n",
    "    for i in range(NUM_NETWORKS):      # Perform NUM_NETWORKS evaluation rounds.\n",
    "        hidden_neurons = []\n",
    "        for _ in range(NUM_HIDDEN_NEURONS):  # Combine NUM_HIDDEN_NEURONS in a network.\n",
    "            candidate = random.choice(population)\n",
    "            candidate.evaluations += 1\n",
    "            hidden_neurons.append(candidate)\n",
    "\n",
    "        network = generate_network_SANE(hidden_neurons) # Generate and evaluate networks.\n",
    "        fitness = evaluate_agent(network, randomise)\n",
    "        evaluations += 1\n",
    "\n",
    "        if fitness > best:\n",
    "            best = fitness\n",
    "\n",
    "        if fitness == MAX_STEPS:\n",
    "            return best, evaluations\n",
    "\n",
    "        # Assign the obtained fitness values to the chromosomes.\n",
    "        for neuron in hidden_neurons:\n",
    "            neuron.fitness += fitness\n",
    "\n",
    "    # Compute the final fitness values by averaging over the number of evaluations\n",
    "    for individual in population:\n",
    "        if individual.evaluations > 0:\n",
    "            individual.fitness = individual.fitness / individual.evaluations\n",
    "        else:\n",
    "            individual.fitness = 0\n",
    "\n",
    "    return best, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a9cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "population = [generate_random_gene_sane(NUM_CONNECTIONS) for i in range(POPULATION_SIZE)]\n",
    "evaluate_population_sane(population, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793b8b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the Pole-Balancing Task - SANE\n",
    "\n",
    "With everything in place, we can finally implement our neuroevolution algorithm to solve the pole-balancing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcbaa1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUM_NETWORKS, POPULATION_SIZE, NUM_HIDDEN_NEURONS, NUM_CONNECTIONS = 200, 200, 8, 5\n",
    "MUTATION_RATE, MUTATION_POWER = 0.1, 3\n",
    "NUM_PARENTS, MAX_GENERATIONS = 50, 1000\n",
    "\n",
    "def solve_pole_sane(random_starts: bool, print_log: bool) -> (list[int], int):\n",
    "    # Initialise population\n",
    "    population = [generate_random_gene_sane(NUM_CONNECTIONS) for _ in range(POPULATION_SIZE)]\n",
    "\n",
    "    num_generation = 0\n",
    "    evaluations = 0\n",
    "    fitness_values = []\n",
    "    while num_generation < MAX_GENERATIONS:\n",
    "        best_fitness, evaluations = evaluate_population_sane(population, evaluations, random_starts)\n",
    "        fitness_values.append(best_fitness)\n",
    "\n",
    "        if best_fitness == MAX_STEPS:\n",
    "            print(f\"SANE found solution in generation {num_generation} after {evaluations} evaluations\")\n",
    "            break\n",
    "\n",
    "        if print_log:\n",
    "            print(f\"Generation {num_generation}\")\n",
    "            print(f\"Best fitness {best_fitness}\")\n",
    "\n",
    "        population = evolve_sane(population)\n",
    "        num_generation += 1\n",
    "    return fitness_values, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc355ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fitness_values, _ = solve_pole_sane(False, False)\n",
    "x_range = np.arange(0, len(fitness_values), 1)\n",
    "\n",
    "plt.plot(x_range, fitness_values)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf437a82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation of SANE\n",
    "\n",
    "For both scenarios (static and random start), we repeat the experiment a couple of times and measure the average fitness of the best-performing network over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3735d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sane(random_start: bool, repetitions: int) -> (list[int], list[int]):\n",
    "    fitness_vals = []\n",
    "    evaluations = []\n",
    "    for _ in range(repetitions):\n",
    "        fitness, evals = solve_pole_sane(random_start, False)\n",
    "        fitness_vals.append(fitness)\n",
    "        evaluations.append(evals)\n",
    "\n",
    "    return fitness_vals, evaluations\n",
    "\n",
    "\n",
    "def plot(fitness_values: list[list[int]]):\n",
    "    # Padding\n",
    "    max_generation = max([len(f) for f in fitness_values])\n",
    "    for f in fitness_values:\n",
    "        while len(f) < max_generation:\n",
    "            f.append(MAX_STEPS)\n",
    "\n",
    "    mean_fitness_time = np.array(fitness_values).mean(axis=0)\n",
    "    x_range = np.arange(0, max_generation, 1)\n",
    "    plt.plot(x_range, mean_fitness_time)\n",
    "    plt.yscale(\"log\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d683c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "repetitions = 3\n",
    "random_start = True\n",
    "fitness, evaluation = evaluate_sane(random_start, repetitions)\n",
    "plot(fitness)\n",
    "print(f\"Average number of evaluations SANE: {mean(evaluation)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
