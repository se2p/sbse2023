{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a399ddc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neuroevolution\n",
    "\n",
    "Neuroevolution uses evolutionary algorithms to optimise neural networks. Before we start with the implementation of neural networks, let's import required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff2efa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43222b5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks consist of neurons and weighted connections between these neurons. Each neuron represents a processing unit in which an activation function is applied to the weighted sum of all incoming connections. After a neuron has been activated, its activation signal is further propagated into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e527130",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Basic neuron definition from which all neuron genes will be derived.\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        self.uid = uid\n",
    "        self.signal_value = 0\n",
    "        self.activation_value = 0\n",
    "        self.incoming_connections = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2011d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input Neuron\n",
    "\n",
    "Input neurons receive a signal from the environment (input feature) and propagate it into the network. Since we are interested in the raw input signal, we refrain from applying any activation functions within the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a9ec6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class InputNeuron(Neuron):\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        super().__init__(uid)\n",
    "\n",
    "    def activate(self) -> float:\n",
    "        self.activation_value = self.signal_value\n",
    "        return self.activation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556593a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Neuron\n",
    "\n",
    "Hidden neurons reside between input and output neurons. They increase the capacity of a neural network, in other words, we require more hidden neurons for complex tasks than for simple ones. The number and distribution of neurons is an optimisation task on its own and one of the main reasons to use neuroevolution. As an activation function, we will use the sigmoid function, which maps any input value to $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78a91b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: float):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "x_range = np.arange(-10, 10, 0.01)\n",
    "sigmoid_values = [sigmoid(x) for x in x_range]\n",
    "plt.plot(x_range, sigmoid_values)\n",
    "plt.title('Sigmoid function', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b3e95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenNeuron(Neuron):\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        super().__init__(uid)\n",
    "\n",
    "    def activate(self) -> float:\n",
    "        self.activation_value = sigmoid(self.signal_value)\n",
    "        return self.activation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11221bcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output Neuron\n",
    "\n",
    "For our output neurons, we have to choose an appropriate activation function that matches the given task. Later, we will try to solve a binary decision problem in which a robot can move to the right or left. Hence, we could use a sigmoid function, whose output is treated as a probability for choosing one of the two classes. However, due to the neuroevolution algorithm we will implement, we add two output neurons to the output layer, assign one output neuron to each class, and choose the class whose output neuron has the highest activation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cea3b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class OutputNeuron(Neuron):\n",
    "\n",
    "    def __init__(self, uid: str):\n",
    "        super().__init__(uid)\n",
    "        self.uid = uid\n",
    "\n",
    "    def activate(self) -> float:\n",
    "        self.activation_value = self.signal_value\n",
    "        return self.activation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5dd04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connections\n",
    "\n",
    "Our neurons are rather useless as long as they are not connected. We define connections by their source and destination neurons. Furthermore, we assign a weight to each connection to specify the strength of a link between two neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5f153",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Connection:\n",
    "\n",
    "    def __init__(self, source: Neuron, target: Neuron, weight: float):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.weight = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5183937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Network Definition\n",
    "\n",
    "With our neurons and connections defined, we can now assemble both components in layers to build a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac436a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers: dict[float, list[Neuron]], connections: list[Connection]):\n",
    "        self.layers = layers\n",
    "        self.connections = connections\n",
    "        self.generate()\n",
    "\n",
    "    # Traverse list of connections and add incoming connections to neurons.\n",
    "    def generate(self):\n",
    "        for connection in self.connections:\n",
    "            connection.target.incoming_connections.append(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5de9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's build a network with five input neurons, three hidden neurons and two output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099dec1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gen_simple_network() -> Network:\n",
    "    input_neurons = [InputNeuron(f\"I{x}\") for x in range(5)]\n",
    "    hidden_neurons = [HiddenNeuron(f\"H{x}\") for x in range(3)]\n",
    "    output_neurons = [OutputNeuron(f\"O{x}\") for x in range(5)]\n",
    "\n",
    "    # Assemble in layers with 0 representing the input layer and 1 the output layer.\n",
    "    layers = {0: input_neurons, 0.5: hidden_neurons, 1: output_neurons}\n",
    "\n",
    "    # Generate connections from every input neuron to each hidden neuron\n",
    "    connections = []\n",
    "    for input_neuron in input_neurons:\n",
    "        for hidden_neuron in hidden_neurons:\n",
    "            weight = random.uniform(-1, 1)\n",
    "            connections.append(Connection(input_neuron, hidden_neuron, weight))\n",
    "\n",
    "    # Generate connections from every hidden neuron to each output neuron\n",
    "    for hidden_neuron in hidden_neurons:\n",
    "        for output_neuron in output_neurons:\n",
    "            weight = random.uniform(-1, 1)\n",
    "            connections.append(Connection(hidden_neuron, output_neuron, weight))\n",
    "\n",
    "    return Network(layers, connections)\n",
    "\n",
    "\n",
    "net = gen_simple_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19d54b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can visualise our generated network using the Graphviz library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc6b53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Network(Network):\n",
    "    def show(self) -> Digraph:\n",
    "        dot = Digraph(graph_attr={'rankdir': 'BT', 'splines': \"line\"})\n",
    "        # Use sub graphs to position input neurons are at the bottom and output neurons at the top.\n",
    "        input_graph = Digraph(graph_attr={'rank': 'min', 'splines': \"line\"})\n",
    "        output_graph = Digraph(graph_attr={'rank': 'max', 'splines': \"line\"})\n",
    "        hidden_graph = Digraph(graph_attr={'splines': \"line\"})\n",
    "\n",
    "        # Traverse network from input to output layer and assign neurons to corresponding subgraph.\n",
    "        layer_keys = list(self.layers.keys())\n",
    "        layer_keys.sort()\n",
    "        for layer in layer_keys:\n",
    "            for neuron in self.layers.get(layer):\n",
    "                if layer == 0:\n",
    "                    input_graph.node(neuron.uid, color='black', fillcolor='white', style='filled')\n",
    "                elif layer == 1:\n",
    "                    output_graph.node(neuron.uid, color='black', fillcolor='white', style='filled')\n",
    "                else:\n",
    "                    hidden_graph.node(neuron.uid, color='black', fillcolor='white', style='filled')\n",
    "\n",
    "        # Combine the sub graphs to a single graph\n",
    "        dot.subgraph(input_graph)\n",
    "        dot.subgraph(hidden_graph)\n",
    "        dot.subgraph(output_graph)\n",
    "        # Link the nodes based on the connection gene.\n",
    "        for connection in self.connections:\n",
    "            dot.edge(connection.source.uid, connection.target.uid,\n",
    "                     label=str(round(connection.weight, 2)), style='solid')\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be62db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gen_simple_network().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f2f54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have our network structure in place, we can implement the network activation that takes an input signal as an argument and outputs the result of the network after it has been activated. We can realise a network's activation by computing our neurons' activation values sequentially from the input to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292455e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Network(Network):\n",
    "    def activate(self, inputs: list[float]) -> list[float]:\n",
    "        # Reset neuron values from previous executions\n",
    "        for neuron_layer in self.layers.values():\n",
    "            for neuron in neuron_layer:\n",
    "                neuron.signal_value = 0\n",
    "\n",
    "        # Load input features into input neurons\n",
    "        input_neurons = self.layers.get(0)\n",
    "        for i in range(len(inputs)):\n",
    "            input_neurons[i].signal_value = inputs[i]\n",
    "            input_neurons[i].activate()\n",
    "\n",
    "        # Traverse the neurons of our network sequentially starting from the hidden layer.\n",
    "        layer_keys = list(self.layers.keys())\n",
    "        layer_keys.sort()\n",
    "        for layer in layer_keys:\n",
    "            for neuron in self.layers.get(layer):\n",
    "                if layer > 0:\n",
    "                    # Calculate weighted sum of incoming connections\n",
    "                    for connection in neuron.incoming_connections:\n",
    "                        neuron.signal_value += connection.source.activation_value * connection.weight\n",
    "                neuron.activate()\n",
    "\n",
    "        output_neurons = self.layers.get(1)\n",
    "        return [o.activation_value for o in output_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5157a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gen_simple_network().activate([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2d42c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverted Pendulum Problem\n",
    "\n",
    "We will use our networks to solve the inverted pendulum (= pole-balancing) problem, a well-known benchmark task in the reinforcement learning community. In this problem scenario, a pole is centred on a cart that can be moved to the right and left. Obviously, any movement to the cart also impacts the pole. The task is to move the cart to the left and right such that the pole remains balanced. Whenever the cart position exceeds the boundaries of the track or the pole tips over 12 degrees, the balancing attempt is deemed a failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420a284",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can simulate the cart and pole system using the following two equations that describe the acceleration of the pole $\\ddot{\\theta_t}$ and the cart $\\ddot{p_t}$ at a given point in time $t$:\n",
    "\n",
    "$$\n",
    "\\ddot{p}_t = \\frac{F_t + m_pl(\\dot{\\theta^2_t} sin\\theta_t - \\ddot{\\theta_t} \\; cos\\theta_t)}{m} \\; \\; \\; \\; \\; \\; \\; \\; (1) \\\\\n",
    "\\ddot{\\theta}_t = \\frac{mg \\; sin\\theta_t - cos\\theta_t(F_t + m_pl\\dot{\\theta}^2_t \\; sin\\theta_t)}{\\frac{4}{3}ml - m_pl \\; cos^2(\\theta_t)} \\; \\; \\; \\; \\; \\; \\; \\; (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40cd42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "where\n",
    "- $p$: Position of the cart\n",
    "- $\\dot{p}$: Velocity of the cart\n",
    "- $\\ddot{p}$: Acceleration of the cart\n",
    "- $\\theta$: Angle of the pole\n",
    "- $\\dot{\\theta}$: Angular velocity of the pole\n",
    "- $\\ddot{\\theta}$: Angular acceleration of the pole\n",
    "- $l$: Length of the pole\n",
    "- $m_p$: Mass of the pole\n",
    "- $m$: Mass of the pole and cart\n",
    "- $F$: Force applied to the cart\n",
    "- $g$: Gravity acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cfbd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our simulation, we will update both systems using the discrete-time equations\n",
    "\n",
    "$$\n",
    "p(t+1) = p(t) + r \\dot{p}(t) \\; (3) \\\\\n",
    "\\theta(t+1) = \\theta(t) + r \\dot{\\theta}(t) \\;(4) \\\\\n",
    "\\dot{p}(t+1) = \\dot{p}(t) + r \\ddot{p}(t) \\; (5) \\\\\n",
    "\\dot{\\theta}(t+1) = \\dot{\\theta}(t) + r \\ddot{\\theta}(t) \\; (6)\n",
    "$$\n",
    "\n",
    "with the discrete time step $r$ set to 0.02 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e10fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "GRAVITY = 9.8  # m/s^2\n",
    "MASS_CART = 1.0  # kg\n",
    "MASS_POLE = 0.1  # kg\n",
    "TOTAL_MASS = (MASS_CART + MASS_POLE)  # kg\n",
    "POLE_LENGTH = 0.5  # m\n",
    "POLEMASS_LENGTH = (MASS_POLE * POLE_LENGTH)\n",
    "FORCE = 10  # N\n",
    "STEP_SIZE = 0.02  # sec\n",
    "FOURTHIRDS = 1.333333\n",
    "\n",
    "\n",
    "def cart_pole_step(action: int, p: float, p_vel: float, theta: float, theta_vel: float) -> (float, float, float, float):\n",
    "    force_dir = FORCE if action > 0 else -FORCE\n",
    "\n",
    "    cos_theta = math.cos(theta)\n",
    "    sin_theta = math.sin(theta)\n",
    "\n",
    "    temp = (force_dir + POLEMASS_LENGTH * theta_vel * theta_vel * sin_theta) / TOTAL_MASS\n",
    "\n",
    "    # Equation 2\n",
    "    theta_acc = (GRAVITY * sin_theta - cos_theta * temp) / (\n",
    "                POLE_LENGTH * (FOURTHIRDS - MASS_POLE * cos_theta * cos_theta / TOTAL_MASS))\n",
    "    # Equation 1\n",
    "    p_acc = temp - POLEMASS_LENGTH * theta_acc * cos_theta / TOTAL_MASS\n",
    "\n",
    "    # Compute new states\n",
    "    p = p + STEP_SIZE * p_vel  # Equation 3\n",
    "    theta = theta + STEP_SIZE * theta_vel  # Equation 4\n",
    "    p_vel = p_vel + STEP_SIZE * p_acc  # Equation 5\n",
    "    theta_vel = theta_vel + STEP_SIZE * theta_acc  # Equation 6\n",
    "\n",
    "    return p, p_vel, theta, theta_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2e238",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test our cart pole system by executing a few steps starting from a clean state where all state variables are set to zero. Given a balanced pole ($\\theta = 0$), we expect the pole to always move in the opposite direction of the applied force. Moreover, we expect increasing velocity values as long as we apply a force in the same direction and a decrease in velocity as soon as the force direction changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7a8e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p = 0\n",
    "p_vel = 0\n",
    "theta = 0\n",
    "theta_vel = 0\n",
    "# Move the cart for 10 steps to the right.\n",
    "for i in range(10):\n",
    "    [p, p_vel, theta, theta_vel] = cart_pole_step(1, p, p_vel, theta, theta_vel)\n",
    "    print(f\"Iteration right: {i}\\nCart Pos: {p}\\nCart Vel: {p_vel}\\nPole Angle: {theta}\\nPole AVel: {theta_vel}\")\n",
    "    print(\"--------------------\")\n",
    "          \n",
    "# Move the cart for 15 steps to the left.\n",
    "for i in range(15):\n",
    "    [p, p_vel, theta, theta_vel] = cart_pole_step(-1, p, p_vel, theta, theta_vel)\n",
    "    print(f\"Iteration left: {i}\\nCart Pos: {p}\\nCart Vel: {p_vel}\\nPole Angle: {theta}\\nPole AVel: {theta_vel}\")\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae558a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can formulate our pole-balancing task as a reinforcement learning problem by implementing a simulation in which an agent's goal is to balance the pole for as long as possible. In our scenario, our agents are neural networks that decide in which direction the cart should be moved at a given state $[p, \\theta, \\dot{p}, \\dot{\\theta}]$. Since we have four input features, the networks will have four input neurons. Furthermore, these input features will be normalised over the following ranges:\n",
    "\n",
    "$$\n",
    "p: [-2.4, 2.4] \\\\\n",
    "\\dot{p}: [-1.5, 1.5] \\\\\n",
    "\\theta: [-12, 12] \\\\\n",
    "\\dot{\\theta}: [-60, 60] \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158d206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We model the pole-balancing problem as a binary classification task using two output neurons, each representing one action. An action is chosen by selecting the action that belongs to the neuron with the highest activation value. Once the pole tips over 12 degrees or the 4.8 meter track is left by the cart, the trial ends. Finally, an agent's performance is measured by the number of steps it managed to survive. A balancing attempt is deemed successful whenever an agent balances the pole for 120,000 time steps, which is equivalent to 40 minutes in real time.\n",
    "\n",
    "For a more challenging second scenario, we add an option that randomises the starting positions of the four input states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93bb35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 120000\n",
    "TWELVE_DEGREES = 0.2094395  #conversion to rad (12*pi)/180\n",
    "\n",
    "def evaluate_agent(network: Network, random_start=False) -> int:\n",
    "    # Define starting state\n",
    "    if random_start:\n",
    "        p = random.uniform(-2.4, 2.4)  # -2.4 < p < 2.4\n",
    "        p_vel = random.uniform(-1.5, 1.5)  # -1.5 < p_acc < 1.5\n",
    "        theta = random.uniform(-TWELVE_DEGREES, TWELVE_DEGREES)  # -12 < theta < 12\n",
    "        theta_vel = random.uniform(-1, 1)  # -60 < theta_acc < 60 (in rad)\n",
    "    else:\n",
    "        p, p_vel, theta, theta_vel = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # Simulation loop\n",
    "    steps = 0\n",
    "    while steps < MAX_STEPS:\n",
    "        # Normalise inputs\n",
    "        inputs = [None] * 5\n",
    "        inputs[0] = (p + 2.4) / 4.8\n",
    "        inputs[1] = (p_vel + 1.5) / 3\n",
    "        inputs[2] = (theta + TWELVE_DEGREES) / (2 * TWELVE_DEGREES)\n",
    "        inputs[3] = (theta_vel + 1.0) / 2.0\n",
    "        inputs[4] = 0.5\n",
    "\n",
    "        # Activate the network and interpret the output, and advance the state.\n",
    "        net_output = network.activate(inputs)\n",
    "        action = -1 if net_output[0] > net_output[1] else 1\n",
    "        p, p_vel, theta, theta_vel = cart_pole_step(action, p, p_vel, theta, theta_vel)\n",
    "\n",
    "        # Check if the attempt is still valid\n",
    "        if p < -2.4 or p > 2.4 or theta < -TWELVE_DEGREES or theta > TWELVE_DEGREES:\n",
    "            return steps\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    # At this point the agent survived for the maximum number of steps\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbea89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a sample network and test it in the pole-balancing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef2138",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = gen_simple_network()\n",
    "evaluate_agent(net, random_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b5be5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As expected, our randomly generated networks could perform better. Therefore, we will now implement a Neuroevolution algorithm to optimise our networks and solve the pole-balancing task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef8917",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SANE (Symbiotic Adaptive Neuro-Evolution)\n",
    "\n",
    "[SANE](https://link.springer.com/article/10.1023/A:1018004120707) implements a symbiotic evolution approach, which means that it does not evolve complete solutions but parts thereof. In the case of SANE, we maintain a population of hidden neurons, which are then combined to form a neural network based on a pre-defined input and output layer. Symbiotic approaches aim to avoid premature convergence by enforcing diverse populations and specialisation of individuals in specific subtasks within the problem environment. Each individual receives a fitness score based on the average fitness of the networks in which they participated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Genetic Representation\n",
    "\n",
    "Each neuron is defined by a label and weight list. The former defines the neuron from/to which the connection is coming/going, and the latter the corresponding link weight. We follow the original SANE implementation of Moriarty and Mikkulainen and define an upper bound of 254 for our label values. We connect from the input layer to the hidden neuron if the label value is smaller than 127, and otherwise, from the hidden neuron to the output layer. Moreover, we apply modulo calculus over the total number of input/output neurons to decide to which specific input/output neuron a connection is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaaa0a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "LABEL_BOUND = 254\n",
    "\n",
    "\n",
    "class HiddenNeuronGene:\n",
    "    hidden_neuron_counter = 0\n",
    "\n",
    "    def __init__(self, labels: list[int], weights: list[float]):\n",
    "        self.labels = [l % LABEL_BOUND for l in labels]  # Enforce upper label boundary.\n",
    "        self.weights = weights\n",
    "        self.fitness = 0\n",
    "        self.evaluations = 0\n",
    "        self.hidden_neuron = HiddenNeuron(\"H\" + str(HiddenNeuronGene.hidden_neuron_counter))\n",
    "        HiddenNeuronGene.hidden_neuron_counter += 1\n",
    "\n",
    "    def generate_connections(self, input_neurons: list[Neuron], output_neurons: list[Neuron]) -> list[Connection]:\n",
    "        connections = []\n",
    "        for label, weight in zip(self.labels, self.weights):\n",
    "\n",
    "            # Generate connection to input layer\n",
    "            if label < 127:\n",
    "                neuron_id = \"I\" + str(label % len(input_neurons))\n",
    "                in_neuron = next((i for i in input_neurons if i.uid == neuron_id), None)\n",
    "                connections.append(Connection(in_neuron, self.hidden_neuron, weight))\n",
    "\n",
    "            # Generate connection to output layer\n",
    "            else:\n",
    "                neuron_id = \"O\" + str(label % len(output_neurons))\n",
    "                out_neuron = next((o for o in output_neurons if o.uid == neuron_id), None)\n",
    "                connections.append(Connection(self.hidden_neuron, out_neuron, weight))\n",
    "\n",
    "        return connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231c320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test our gene implementation by generating a neural network using three randomly generated HiddenNeuronGenes, each realising five connections. Moreover, we implement a helper function to create chromosomes with random gene values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf50729",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def generate_random_gene_sane(num_connections: int) -> HiddenNeuronGene:\n",
    "    labels = []\n",
    "    weights = []\n",
    "    for _ in range(num_connections):\n",
    "        labels.append(math.floor(random.uniform(0, LABEL_BOUND)))\n",
    "        weights.append(random.uniform(-1, 1))\n",
    "    return HiddenNeuronGene(labels, weights)\n",
    "\n",
    "\n",
    "def generate_network_SANE(chromosomes: list[HiddenNeuronGene]) -> Network:\n",
    "    input_neurons = [InputNeuron(f\"I{x}\") for x in range(5)]\n",
    "    output_neurons = [OutputNeuron(f\"O{x}\") for x in range(2)]\n",
    "\n",
    "    # Extract the hidden neurons and their connections from the provided chromosomes.\n",
    "    hidden_neurons = []\n",
    "    connections = []\n",
    "    for chromosome in chromosomes:\n",
    "        hidden_neurons.append(chromosome.hidden_neuron)\n",
    "        connections.extend(chromosome.generate_connections(input_neurons, output_neurons))\n",
    "\n",
    "    # Assemble in layers.\n",
    "    layers = {0: input_neurons, 0.5: hidden_neurons, 1: output_neurons}\n",
    "\n",
    "    return Network(layers, connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8349d56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "genes = [generate_random_gene_sane(5) for x in range(3)]\n",
    "net = generate_network_SANE(genes)\n",
    "net.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343f868",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Crossover\n",
    "\n",
    "SANE primarily breeds offspring using a single-point crossover operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f8c88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crossover_sane(parent_1: HiddenNeuronGene, parent_2: HiddenNeuronGene) -> [HiddenNeuronGene, HiddenNeuronGene]:\n",
    "    # Extract the genes from both parents\n",
    "    label_gene_1, weight_gene_1 = parent_1.labels, parent_1.weights\n",
    "    label_gene_2, weight_gene_2 = parent_2.labels, parent_2.weights\n",
    "\n",
    "    # Determine the crossover point. We assume both parent genes have the same gene lengths.\n",
    "    crossover_point = math.floor(random.uniform(1, len(label_gene_1)))\n",
    "\n",
    "    # Apply the crossover operation\n",
    "    child_1_labels = label_gene_1[:crossover_point] + label_gene_2[crossover_point:]\n",
    "    child_1_weights = weight_gene_1[:crossover_point] + weight_gene_2[crossover_point:]\n",
    "    child_2_labels = label_gene_2[:crossover_point] + label_gene_1[crossover_point:]\n",
    "    child_2_weights = weight_gene_2[:crossover_point] + weight_gene_1[crossover_point:]\n",
    "\n",
    "    # Breed and return the two children.\n",
    "    return [HiddenNeuronGene(child_1_labels, child_1_weights), HiddenNeuronGene(child_2_labels, child_2_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ce61a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "parent_1 = generate_random_gene_sane(5)\n",
    "parent_2 = generate_random_gene_sane(5)\n",
    "parent_1.weights = [round(w * 100) / 100 for w in parent_1.weights]  # Round weights for better visualisation\n",
    "parent_2.weights = [round(w * 100) / 100 for w in parent_2.weights]\n",
    "[child_1, child_2] = crossover_sane(parent_1, parent_2)\n",
    "\n",
    "print(\"Neuron \\t\\t| Labels \\t\\t\\t| Weights\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(f\"Parent 1 \\t| {parent_1.labels} \\t| {parent_1.weights}\")\n",
    "print(f\"Parent 2 \\t| {parent_2.labels} \\t| {parent_2.weights}\")\n",
    "print(f\"Child 1 \\t| {child_1.labels} \\t| {child_1.weights}\")\n",
    "print(f\"Child 2 \\t| {child_2.labels} \\t| {child_2.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45272a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mutation\n",
    "\n",
    "The mutation operator adds gaussian noise to the values of the two gene lists.\n",
    " \n",
    "In contrast, to other evolutionary algorithms, SANE uses mutation only to add genetic material that may be missing from the initial population. Thus, mutation is not responsible for creating diversity within the population as diversity is already present due to the symbiotic evolution approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849eff1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "MUTATION_POWER = 2\n",
    "\n",
    "\n",
    "def mutate_sane(parent: HiddenNeuronGene) -> HiddenNeuronGene:\n",
    "    mutated_labels = []\n",
    "    mutated_weights = []\n",
    "    # Add gaussian noise to the values.\n",
    "    for label, weight in zip(parent.labels, parent.weights):\n",
    "        mutated_labels.append(math.floor(np.random.normal(label, MUTATION_POWER)))\n",
    "        mutated_weights.append(np.random.normal(weight, MUTATION_POWER / 10))\n",
    "\n",
    "    return HiddenNeuronGene(mutated_labels, mutated_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f8634",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "parent = generate_random_gene_sane(5)\n",
    "mutant = mutate_sane(parent)\n",
    "parent.weights = [round(w * 100) / 100 for w in parent.weights]  # Round weights for better visualisation\n",
    "mutant.weights = [round(w * 100) / 100 for w in mutant.weights]\n",
    "print(\"Neuron \\t|\\t\\t Labels \\t\\t|\\t\\t Weights\")\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(f\"Parent \\t|\\t {parent.labels} \\t|\\t {parent.weights}\")\n",
    "print(f\"Mutant \\t|\\t {mutant.labels} \\t|\\t {mutant.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8c91b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evolution\n",
    "\n",
    "A new generation is formed by applying crossover to the best-performing members of the current population. Parents are selected sequentially, and mating partners must have a fitness value at least as high as the selected parent. Finally, we apply mutation to fill the remaining slots in our population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6756062",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUM_PARENTS = 50\n",
    "POPULATION_SIZE = 200\n",
    "\n",
    "\n",
    "def evolve_sane(population: list[HiddenNeuronGene]) -> HiddenNeuronGene:\n",
    "    # Sort the population such that the best-performing chromosomes are at the beginning.\n",
    "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "    offspring = []\n",
    "\n",
    "    # Crossover\n",
    "    for i in range(NUM_PARENTS):\n",
    "        first_parent = population[i]\n",
    "        second_parent = population[math.floor(random.uniform(0, i))]\n",
    "        offspring.extend(crossover_sane(first_parent, second_parent))\n",
    "\n",
    "    # Mutation\n",
    "    while len(offspring) < POPULATION_SIZE:\n",
    "        offspring.append(mutate_sane(random.choice(population)))\n",
    "\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11c302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "population = [generate_random_gene_sane(5) for i in range(POPULATION_SIZE)]\n",
    "offspring = evolve_sane(population)\n",
    "print(f\"Parent Population Size: {len(population)}\")\n",
    "print(f\"Offspring Population Size: {len(offspring)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88791430",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitness evaluation\n",
    "\n",
    "Since our population consists of partial solutions and not full-fledged neural networks, we have to combine several chromosomes into a neural network for each evaluation episode. The fitness of a single chromosome is set to the average fitness of all networks in which it participated. For our pole-balancing task, we combine eight chromosomes, each realising five connections. Hence, we obtain networks that have a total of 40 connections. In each iteration, we generate 200 networks while maintaining a neuron population size of 200, which means that, on average, each neuron is allowed to participate in eight networks per generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b810d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_NETWORKS, POPULATION_SIZE, NUM_HIDDEN_NEURONS, NUM_CONNECTIONS = 200, 200, 8, 5\n",
    "\n",
    "def evaluate_population_sane(population: list[HiddenNeuronGene], evaluations: int, randomise=False) -> (int, int):\n",
    "    for c in population:     # Reset fitness and evaluation counter for each neuron\n",
    "        c.fitness = 0\n",
    "        c.evaluations = 0\n",
    "\n",
    "    best = 0\n",
    "    for i in range(NUM_NETWORKS):      # Perform NUM_NETWORKS evaluation rounds.\n",
    "        hidden_neurons = []\n",
    "        for _ in range(NUM_HIDDEN_NEURONS):  # Combine NUM_HIDDEN_NEURONS in a network.\n",
    "            candidate = random.choice(population)\n",
    "            candidate.evaluations += 1\n",
    "            hidden_neurons.append(candidate)\n",
    "\n",
    "        network = generate_network_SANE(hidden_neurons) # Generate and evaluate networks.\n",
    "        fitness = evaluate_agent(network, randomise)\n",
    "        evaluations += 1\n",
    "\n",
    "        if fitness > best:\n",
    "            best = fitness\n",
    "\n",
    "        if fitness == MAX_STEPS:\n",
    "            return best, evaluations\n",
    "\n",
    "        # Assign the obtained fitness values to the chromosomes.\n",
    "        for neuron in hidden_neurons:\n",
    "            neuron.fitness += fitness\n",
    "\n",
    "    # Compute the final fitness values by averaging over the number of evaluations\n",
    "    for individual in population:\n",
    "        if individual.evaluations > 0:\n",
    "            individual.fitness = individual.fitness / individual.evaluations\n",
    "        else:\n",
    "            individual.fitness = 0\n",
    "\n",
    "    return best, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d20426",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "population = [generate_random_gene_sane(NUM_CONNECTIONS) for i in range(POPULATION_SIZE)]\n",
    "evaluate_population_sane(population, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18db78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the Pole-Balancing Task - SANE\n",
    "\n",
    "With everything in place, we can finally implement our neuroevolution algorithm to solve the pole-balancing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812544de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NUM_NETWORKS, POPULATION_SIZE, NUM_HIDDEN_NEURONS, NUM_CONNECTIONS = 200, 200, 8, 5\n",
    "MUTATION_RATE, MUTATION_POWER = 0.1, 3\n",
    "NUM_PARENTS, MAX_GENERATIONS = 50, 1000\n",
    "\n",
    "def solve_pole_sane(random_starts: bool, print_log: bool) -> (list[int], int):\n",
    "    # Initialise population\n",
    "    population = [generate_random_gene_sane(NUM_CONNECTIONS) for _ in range(POPULATION_SIZE)]\n",
    "\n",
    "    num_generation = 0\n",
    "    evaluations = 0\n",
    "    fitness_values = []\n",
    "    while num_generation < MAX_GENERATIONS:\n",
    "        best_fitness, evaluations = evaluate_population_sane(population, evaluations, random_starts)\n",
    "        fitness_values.append(best_fitness)\n",
    "\n",
    "        if best_fitness == MAX_STEPS:\n",
    "            print(f\"SANE found solution in generation {num_generation} after {evaluations} evaluations\")\n",
    "            break\n",
    "\n",
    "        if print_log:\n",
    "            print(f\"Generation {num_generation}\")\n",
    "            print(f\"Best fitness {best_fitness}\")\n",
    "\n",
    "        population = evolve_sane(population)\n",
    "        num_generation += 1\n",
    "    return fitness_values, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173637f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fitness_values, _ = solve_pole_sane(False, False)\n",
    "x_range = np.arange(0, len(fitness_values), 1)\n",
    "\n",
    "plt.plot(x_range, fitness_values)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44477c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation of SANE\n",
    "\n",
    "For both scenarios (static and random start), we repeat the experiment a couple of times and measure the average fitness of the best-performing network over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c3a92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sane(random_start: bool, repetitions: int) -> (list[int], list[int]):\n",
    "    fitness_vals = []\n",
    "    evaluations = []\n",
    "    for _ in range(repetitions):\n",
    "        fitness, evals = solve_pole_sane(random_start, False)\n",
    "        fitness_vals.append(fitness)\n",
    "        evaluations.append(evals)\n",
    "\n",
    "    return fitness_vals, evaluations\n",
    "\n",
    "\n",
    "def plot(fitness_values: list[list[int]]):\n",
    "    # Padding\n",
    "    max_generation = max([len(f) for f in fitness_values])\n",
    "    for f in fitness_values:\n",
    "        while len(f) < max_generation:\n",
    "            f.append(MAX_STEPS)\n",
    "\n",
    "    mean_fitness_time = np.array(fitness_values).mean(axis=0)\n",
    "    x_range = np.arange(0, max_generation, 1)\n",
    "    plt.plot(x_range, mean_fitness_time)\n",
    "    plt.yscale(\"log\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87824da6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "repetitions = 3\n",
    "random_start = False\n",
    "fitness, evaluation = evaluate_sane(random_start, repetitions)\n",
    "plot(fitness)\n",
    "print(f\"Average number of evaluations SANE: {mean(evaluation)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44baccd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CoSyNE (Cooperative Synapse Neuroevolution)\n",
    "\n",
    "[CoSyNE](https://nn.cs.utexas.edu/downloads/papers/gomez.jmlr08.pdf) evolves connection weights by encoding all connection weights $m$ of a fixed topology network into subgenes that are distributed over $m$ species. Since each species maintains $n$ genes, we can model our population by a $n$ x $m$ matrix. A functional network can be generated by combining a single gene of each species within a single row of the matrix. \n",
    "\n",
    "The most crucial feature of CoSyNE is the cooperative evolution approach realised by permuting the rows of a single subpopulation, leading to new combinations of the subgenes and new networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a519a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Genetic Representation\n",
    "\n",
    "We assign each subgene of CoSyNE a weight and fitness value. The former defines the weight for a corresponding connection and the latter the achieved fitness of the network in which the gene participated. Finally, each gene has a permutation attribute that defines the probability of being permutated in the cooperative evolution step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e568e18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class WeightGene:\n",
    "\n",
    "    def __init__(self, weight, fitness=0):\n",
    "        self.weight = weight\n",
    "        self.fitness = fitness\n",
    "        self.permutation_prob = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b92bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating the Initial Population\n",
    "\n",
    "We can create a new population by generating $n$ x $m$ weight genes having a random weight value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e581a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "WEIGHT_MAGNITUDE = 1\n",
    "\n",
    "\n",
    "def init_population_COSYNE(num_weights: int, subpop_size: int) -> list[list[WeightGene]]:\n",
    "    population = []\n",
    "    for i in range(subpop_size): # Number of weights per species == Number of Networks in Population\n",
    "        single_network = []\n",
    "        for j in range(num_weights): # Number of Species == Number of weights per Network\n",
    "            single_network.append(WeightGene(random.uniform(-WEIGHT_MAGNITUDE, WEIGHT_MAGNITUDE)))\n",
    "        population.append(single_network)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5943beb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To visualise our population, we implement a helper function that takes a population as input and outputs the respective weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595620f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def show_population_COSYNE(population: list[list[WeightGene]]) -> list[list[float]]:\n",
    "    weights = []\n",
    "    for network in population:\n",
    "        network_weights = []\n",
    "        for weight_gene in network:\n",
    "            network_weights.append(weight_gene.weight)\n",
    "        weights.append(network_weights)\n",
    "    return weights\n",
    "\n",
    "\n",
    "pop = init_population_COSYNE(2, 4)\n",
    "show_population_COSYNE(pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d0dd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight Genes --> Networks\n",
    "\n",
    "Since we have to transform our weights into networks for the evaluation and evolution phase, we implement a function that converts a list of weight genes to the corresponding network. Moreover, because CoSyNE evolves fixed topologies, we have to define the network topology by specifying the number of input, hidden and output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6aeeaf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Network(Network):\n",
    "    def __init__(self, layers: dict[float, list[Neuron]], connections: list[Connection]):\n",
    "        self.layers = layers\n",
    "        self.connections = connections\n",
    "        self.generate()\n",
    "        self.fitness = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2977f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def weights_to_network(weight_genes: list[WeightGene], num_in: int, num_hidden: int, num_out: int) -> Network:\n",
    "    input_neurons = [InputNeuron(f\"I{x}\") for x in range(num_in)]\n",
    "    hidden_neurons = [HiddenNeuron(f\"H{x}\") for x in range(num_hidden)]\n",
    "    output_neurons = [OutputNeuron(f\"O{x}\") for x in range(num_out)]\n",
    "\n",
    "    # Check number of connections\n",
    "    required_connections = num_in * num_hidden + num_hidden * num_out\n",
    "    assert len(weight_genes) == required_connections\n",
    "\n",
    "    # Create connections\n",
    "    weight_index = 0\n",
    "    connections = []\n",
    "    for in_neuron in input_neurons:\n",
    "        for hidden_neuron in hidden_neurons:\n",
    "            connections.append(Connection(in_neuron, hidden_neuron, weight_genes[weight_index].weight))\n",
    "            weight_index += 1\n",
    "    for hidden_neuron in hidden_neurons:\n",
    "        for out_neuron in output_neurons:\n",
    "            connections.append(Connection(hidden_neuron, out_neuron, weight_genes[weight_index].weight))\n",
    "            weight_index += 1\n",
    "\n",
    "    layers = {0: input_neurons, 0.5: hidden_neurons, 1: output_neurons}\n",
    "    network = Network(layers, connections)\n",
    "    network.fitness = weight_genes[0].fitness  # Fitness of all genes is the same\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b17a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "weights = init_population_COSYNE(12, 1)\n",
    "network = weights_to_network(weights[0], 5, 2, 1)\n",
    "network.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b982e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Based on the previous function, we can now transform a whole CoSyNE population into the respective networks by applying the **weights_to_network** function to each row of our population matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bb14e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def weights_to_networks_pop(weights: list[list[WeightGene]], num_in: int, num_hidden: int,\n",
    "                            num_out: int) -> list[Network]:\n",
    "    return [weights_to_network(w, num_in, num_hidden, num_out) for w in weights]\n",
    "\n",
    "\n",
    "weights = init_population_COSYNE(3, 10)\n",
    "networks = weights_to_networks_pop(weights, 2, 1, 1)\n",
    "\n",
    "print(f\"Networks vs. Subpopulations: {len(networks)} ==> {len(weights)}\")\n",
    "print(f\"Connections vs. Num Weights: {len(networks[0].connections)} ==> {len(weights[0])}\")\n",
    "weight_values = [w.weight for w in weights[0]]\n",
    "connection_values = [c.weight for c in networks[0].connections]\n",
    "print(\"\\nConnection Weights vs. Weight Values:\")\n",
    "print(f\"{weight_values}\\n{connection_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c94115",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Networks ---> Weight Genes\n",
    "\n",
    "In order to facilitate the mapping between network and subgene fitness values, we extend the attributes of our networks with a fitness value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b029d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We implement the complement to the former two conversion functions such that we can extract weight genes, including their fitness values, from the respective networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71376f3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def network_to_weights(network: Network) -> list[WeightGene]:\n",
    "    weight_genes = []\n",
    "    for connection in network.connections:\n",
    "        weight_genes.append(WeightGene(connection.weight, network.fitness))\n",
    "    return weight_genes\n",
    "\n",
    "\n",
    "weights = init_population_COSYNE(3, 1)\n",
    "network = weights_to_network(weights[0], 2, 1, 1)\n",
    "net_weights = network_to_weights(network)\n",
    "\n",
    "print(show_population_COSYNE(weights))\n",
    "print(show_population_COSYNE([net_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ea59e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def networks_to_weights_pop(networks: list[Network]) -> list[list[WeightGene]]:\n",
    "    weight_population = []\n",
    "    for network in networks:\n",
    "        network_weights = []\n",
    "        weight_population.append(network_to_weights(network))\n",
    "    return weight_population\n",
    "\n",
    "\n",
    "weights = init_population_COSYNE(3, 1)\n",
    "networks = weights_to_networks_pop(weights, 2, 1, 1)\n",
    "network_weights = networks_to_weights_pop(networks)\n",
    "\n",
    "print(f\"Subpopulation size before/after: {len(weights)} ==> {len(network_weights)}\")\n",
    "print(f\"Weight size before/after: {len(weights[0])} ==> {len(network_weights[0])}\")\n",
    "weight_values_before = [w.weight for w in weights[0]]\n",
    "weight_values_after = [w.weight for w in network_weights[0]]\n",
    "print(f\"Weight values before/after: \\n{weight_values_before}\\n{weight_values_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9aa93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Crossover\n",
    "\n",
    "CoSyNE applies multi-point crossover, where each weight gene and its respective weight value is inherited randomly by one of the two parents. We use a mask consisting of zeroes and ones and multiply the weights of the first parent elementwise with the mask. Then we add the weights of the second parent after multiplying it with the inverted mask. Finally, the second child is produced by switching the parents' positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1993fd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def crossover_cosyne(parent_1: Network, parent_2: Network) -> (Network, Network):\n",
    "    weights_p1 = np.array([w.weight for w in network_to_weights(parent_1)])\n",
    "    weights_p2 = np.array([w.weight for w in network_to_weights(parent_2)])\n",
    "\n",
    "    # Crossover mask decides which elements will be switched\n",
    "    crossover_mask = np.array([1 if random.uniform(0, 1) < 0.5 else 0 for _ in range(len(weights_p1))])\n",
    "\n",
    "    # Form children via element-wise multiplication of crossover_mask and elementwise complement addition with mating parent.\n",
    "    weights_c1 = weights_p1 * crossover_mask + weights_p2 * (1 - crossover_mask)\n",
    "    weights_c2 = weights_p2 * crossover_mask + weights_p1 * (1 - crossover_mask)\n",
    "\n",
    "    weight_genes_c1 = [WeightGene(w) for w in weights_c1]\n",
    "    weight_genes_c2 = [WeightGene(w) for w in weights_c2]\n",
    "\n",
    "    num_in = len(parent_1.layers.get(0))\n",
    "    num_hidden = len(parent_1.layers.get(0.5))\n",
    "    num_out = len(parent_1.layers.get(1))\n",
    "\n",
    "    c1 = weights_to_network(weight_genes_c1, num_in, num_hidden, num_out)\n",
    "    c2 = weights_to_network(weight_genes_c2, num_in, num_hidden, num_out)\n",
    "    return c1, c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd619f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_in = 2\n",
    "num_hidden = 1\n",
    "num_out = 1\n",
    "num_weights = num_in * num_hidden + num_hidden * num_out\n",
    "\n",
    "weights = init_population_COSYNE(num_weights, 2)\n",
    "network_1 = weights_to_network(weights[0], num_in, num_hidden, num_out)\n",
    "network_2 = weights_to_network(weights[1], num_in, num_hidden, num_out)\n",
    "child_1, child_2 = crossover_cosyne(network_1, network_2)\n",
    "\n",
    "print(f\"Parent 1: {[w.weight for w in weights[0]]}\")\n",
    "print(f\"Parent 2: {[w.weight for w in weights[1]]}\")\n",
    "print(f\"Child 1: {[w.weight for w in network_to_weights(child_1)]}\")\n",
    "print(f\"Child 2: {[w.weight for w in network_to_weights(child_2)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03394f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mutation\n",
    "\n",
    "Mutation is applied on the network level and adds noise to the connection weights based on a predefined mutation probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f063663",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "COSYNE_MUTATION_RATE = 0.3\n",
    "COSYNE_MUTATION_POWER = 1\n",
    "\n",
    "\n",
    "def mutate_cosyne(networks: list[Network]):\n",
    "    for network in networks:\n",
    "        for connection in network.connections:\n",
    "            if random.uniform(0, 1) < COSYNE_MUTATION_RATE:\n",
    "                connection.weight = np.random.normal(connection.weight, COSYNE_MUTATION_POWER)\n",
    "\n",
    "\n",
    "num_in = 2\n",
    "num_hidden = 1\n",
    "num_out = 1\n",
    "num_weights = num_in * num_hidden + num_hidden * num_out\n",
    "\n",
    "weights = init_population_COSYNE(num_weights, 1)\n",
    "network = weights_to_network(weights[0], num_in, num_hidden, num_out)\n",
    "print(f\"Parent: {show_population_COSYNE(weights)}\")\n",
    "\n",
    "mutate_cosyne([network])\n",
    "mutant_weights = network_to_weights(network)\n",
    "print(f\"Mutant: {[w.weight for w in mutant_weights]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f16cb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evolution\n",
    "\n",
    "A new generation is formed by inheriting the first half of the best-performing networks. The second half is then generated by mating and mutating the upper quarter of the best-performing networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce5124",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "SUBPOP_SIZE = 100\n",
    "COSYNE_PARENTS = math.floor(0.25 * SUBPOP_SIZE)\n",
    "INPUT_SIZE = 5\n",
    "HIDDEN_SIZE = 3\n",
    "OUTPUT_SIZE = 2\n",
    "NUM_COSYNE_CONNECTIONS = INPUT_SIZE * HIDDEN_SIZE + OUTPUT_SIZE * HIDDEN_SIZE\n",
    "\n",
    "\n",
    "def evolve_cosyne(networks: list[Network]) -> list[Network]:\n",
    "    # Sort networks by fitness\n",
    "    networks.sort(key=lambda x: x.fitness, reverse=True)\n",
    "\n",
    "    # First half based on best-performing networks\n",
    "    offspring = networks[:COSYNE_PARENTS * 2]\n",
    "\n",
    "    # Second half based on children of best-performing networks\n",
    "    mating_children = []\n",
    "    for i in range(COSYNE_PARENTS):\n",
    "        parent_1 = offspring[i]\n",
    "        parent_2 = random.choice(offspring)\n",
    "        mating_children.extend(crossover_cosyne(parent_1, parent_2))\n",
    "\n",
    "    # Add mating children to population\n",
    "    offspring.extend(mating_children)\n",
    "\n",
    "    # Mutate\n",
    "    mutate_cosyne(networks)\n",
    "\n",
    "    return networks\n",
    "\n",
    "\n",
    "weights = init_population_COSYNE(NUM_COSYNE_CONNECTIONS, 100)\n",
    "networks = weights_to_networks_pop(weights, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "print(f\"Old Generation Size: {len(networks)}\")\n",
    "offspring = evolve_cosyne(networks)\n",
    "print(f\"New Generation Size: {len(offspring)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb9211",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coevolution - Permuting The Subgenes\n",
    "\n",
    "The coevolution principle is realised by permuting the row positions of our subgenes within their respective species. For this purpose, we implement two helper functions for extracting and replacing a single column from our matrix population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289ebdf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_column(index: int, matrix:list[list]):\n",
    "    return [row[index] for row in matrix]\n",
    "\n",
    "\n",
    "def replace_column(index: int, matrix:list[list], column:list):\n",
    "    for i, row in enumerate(matrix):\n",
    "        row[index] = column[i]\n",
    "\n",
    "\n",
    "test_matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "print(f\"Before modification {get_column(1, test_matrix)}\")\n",
    "replace_column(1, test_matrix, [-1, 0, 1])\n",
    "print(f\"After modification {get_column(1, test_matrix)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d077a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The permutation algorithm starts by computing the permutation probability of each weight gene based on the equation\n",
    "\n",
    "$$prob(x_{ij}) = 1 - \\sqrt[n]{\\frac{f(x_{ij})-f_i^{min}}{f_i^{max} - f_i^{min}}}$$\n",
    "\n",
    ", where $f(x_{ij})$ corresponds to the fitness of individual $x_{ij}$ and $f_i^{min}$/$f_i^{max}$ to the least and most fit individuals of subpopulation $i$. Therefore, the probability of changing a network at position $j$ is inversely proportional to its fitness, such that good-performing networks are more likely to be preserved, and poor networks are recombined to search for better solutions.\n",
    "\n",
    "Governed by the computed probability, we shift each gene to the nearest neighbour for which randomness determined that permutation should be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a1f94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def permute(weight_population: list[list[WeightGene]]) -> list[list[WeightGene]]:\n",
    "    \n",
    "    # Sort matrix list by fitness of respective networks\n",
    "    networks = weights_to_networks_pop(weight_population, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "    networks.sort(key=lambda x: x.fitness, reverse=True)\n",
    "    weight_population = networks_to_weights_pop(networks)\n",
    "    \n",
    "    for i in range(len(weight_population[0])):   # Iterate over weights in subpopulation\n",
    "        sub_pop = get_column(i, weight_population)\n",
    "\n",
    "        # Calculate probability of permutation for each WeightGene\n",
    "        min_fitness = min([w.fitness for w in sub_pop])\n",
    "        max_fitness = max([w.fitness for w in sub_pop])\n",
    "        for gene in sub_pop:\n",
    "            if max_fitness == min_fitness:\n",
    "                gene.permutation_prob = 1\n",
    "            else:\n",
    "                temp = (gene.fitness - min_fitness) / (max_fitness - min_fitness)\n",
    "                gene.permutation_prob = 1 - math.pow(temp, 1 / len(sub_pop))\n",
    "\n",
    "        # Permute based on determined probabilities\n",
    "        curr = None\n",
    "        first_index = None\n",
    "        for j, gene in enumerate(sub_pop):\n",
    "            if random.uniform(0, 1) < gene.permutation_prob:\n",
    "                if curr is not None:\n",
    "                    tmp = sub_pop[j]\n",
    "                    sub_pop[j] = curr\n",
    "                    curr = tmp\n",
    "                else:\n",
    "                    first_index = j\n",
    "                    curr = gene\n",
    "        sub_pop[first_index] = curr\n",
    "        replace_column(i, weight_population, sub_pop)\n",
    "\n",
    "    return weight_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe4521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_weights = INPUT_SIZE * HIDDEN_SIZE + HIDDEN_SIZE * OUTPUT_SIZE\n",
    "weights = init_population_COSYNE(num_weights, 5)\n",
    "networks = weights_to_networks_pop(weights, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "weights = networks_to_weights_pop(networks)\n",
    "\n",
    "\n",
    "subpop_weights = [w.weight for w in get_column(0, weights)]\n",
    "print(f\"Before: {subpop_weights}\")\n",
    "weights_after = permute(weights)\n",
    "subpop_weights_after = [w.weight for w in get_column(0, weights_after)]\n",
    "print(f\"After: {subpop_weights_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b14fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Population Evaluation\n",
    "\n",
    "Once again, we will evaluate our alogrithm on the pole-balancing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022aeed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_population_cosyne(networks: list[Network], evaluations: int, random_start=False) -> (int, int):\n",
    "    best = 0\n",
    "    for network in networks:\n",
    "        evaluations += 1\n",
    "        fitness = evaluate_agent(network, random_start)\n",
    "        network.fitness = fitness\n",
    "\n",
    "        if fitness == MAX_STEPS:\n",
    "            return fitness, evaluations\n",
    "\n",
    "        if fitness > best:\n",
    "            best = fitness\n",
    "\n",
    "    return best, evaluations\n",
    "\n",
    "\n",
    "weights = init_population_COSYNE(NUM_COSYNE_CONNECTIONS, 100)\n",
    "networks = weights_to_networks_pop(weights, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "best_fitness, _ = evaluate_population_cosyne(networks, 0)\n",
    "best_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754eda14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the Pole-Balancing Task - CoSyNE\n",
    "\n",
    "Finally, we assemble our evaluation, evolution and permutation functions of CoSyNE to solve the pole balancing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db9a35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 5\n",
    "HIDDEN_SIZE = 3\n",
    "OUTPUT_SIZE = 2\n",
    "NUM_COSYNE_CONNECTIONS = INPUT_SIZE * HIDDEN_SIZE + HIDDEN_SIZE * OUTPUT_SIZE\n",
    "SUBPOP_SIZE = 200\n",
    "COSYNE_PARENTS = math.floor(0.25 * SUBPOP_SIZE)\n",
    "COSYNE_MUTATION_RATE = 0.3\n",
    "COSYNE_MUTATION_POWER = 0.3\n",
    "WEIGHT_MAGNITUDE = 1\n",
    "\n",
    "\n",
    "def solve_pole_cosyne(random_starts: bool, print_log: bool) -> (list[int], int):\n",
    "    weight_population = init_population_COSYNE(NUM_COSYNE_CONNECTIONS, SUBPOP_SIZE)\n",
    "    num_generation = 0\n",
    "    evaluations = 0\n",
    "    fitness_values = []\n",
    "\n",
    "    while num_generation < 1000:\n",
    "        network_population = weights_to_networks_pop(weight_population, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "        best_fitness, evaluations = evaluate_population_cosyne(network_population, evaluations, random_starts)\n",
    "        fitness_values.append(best_fitness)\n",
    "\n",
    "        if best_fitness == MAX_STEPS:\n",
    "            print(f\"COSYNE found solution in generation {num_generation} after {evaluations} evaluations\")\n",
    "            break\n",
    "\n",
    "        if print_log:\n",
    "            print(f\"Generation {num_generation}\")\n",
    "            print(f\"Best fitness {best_fitness}\")\n",
    "\n",
    "        network_population = evolve_cosyne(network_population)\n",
    "        weight_population = networks_to_weights_pop(network_population)\n",
    "        weight_population = permute(weight_population)\n",
    "        num_generation += 1\n",
    "\n",
    "    return fitness_values, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392704c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fitness_values, _ = solve_pole_cosyne(False, False)\n",
    "x_range = np.arange(0, len(fitness_values), 1)\n",
    "\n",
    "plt.plot(x_range, fitness_values)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bfeb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation of CoSyNE\n",
    "\n",
    "We evaluate CoSyNE by comparing it against SANE in the static and randomised pole-balancing task using similar hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db81cf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_NE_algorithms(random_start: bool, repetitions: int) -> (int, int, int):\n",
    "    fitness_SANE = []\n",
    "    fitness_COSYNE = []\n",
    "    evaluations_SANE = []\n",
    "    evaluations_COSYNE = []\n",
    "    for _ in range(repetitions):\n",
    "        fitness, evaluations = solve_pole_sane(random_start, False)\n",
    "        fitness_SANE.append(fitness)\n",
    "        evaluations_SANE.append(evaluations)\n",
    "\n",
    "        fitness, evaluations = solve_pole_cosyne(random_start, False)\n",
    "        fitness_COSYNE.append(fitness)\n",
    "        evaluations_COSYNE.append(evaluations)\n",
    "\n",
    "    return fitness_SANE, evaluations_SANE, fitness_COSYNE, evaluations_COSYNE\n",
    "\n",
    "\n",
    "def plot_NE_algorithms(fitness_SANE: list[list[int]], fitness_COSYNE: list[list[int]]):\n",
    "    # Padding\n",
    "    plt.yscale(\"log\")\n",
    "    max_size = max([max([len(f) for f in f_list]) for f_list in [fitness_SANE, fitness_COSYNE]])\n",
    "    for fitness_list in [fitness_SANE, fitness_COSYNE]:\n",
    "        for f in fitness_list:\n",
    "            while len(f) < max_size:\n",
    "                f.append(MAX_STEPS)\n",
    "\n",
    "    labels = [\"SANE\", \"COSYNE\"]\n",
    "    colours = [\"darkorange\", \"royalblue\"]\n",
    "    style = ['solid', 'dotted']\n",
    "    for n, fitness_list in enumerate([fitness_SANE, fitness_COSYNE]):\n",
    "        mean_fitness_time = np.array(fitness_list).mean(axis=0)\n",
    "        x_range = np.arange(0, max_size, 1)\n",
    "        plt.plot(x_range, mean_fitness_time, color=colours[n], label=labels[n], linestyle=style[n])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93856cd",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "repetitions = 10\n",
    "random_start = True\n",
    "f_SANE, eval_SANE, f_COSYNE, eval_COSYNE = evaluate_NE_algorithms(random_start, repetitions)\n",
    "plot_NE_algorithms(f_SANE, f_COSYNE)\n",
    "print(f\"Average number of evaluations SANE: {mean(eval_SANE)}\\nAverage number of evaluations COSYNE: {mean(eval_COSYNE)}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
